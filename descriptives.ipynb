{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AI Exposure and College Enrollment Analysis - 4-DIGIT CIP VERSION\n",
    "==================================================================\n",
    "Updated for:\n",
    "- 4-digit CIP codes (436 programs vs 49 at 2-digit level)\n",
    "- 2019-2025 enrollment data (combined from both files)\n",
    "- More granular analysis (e.g., Computer Science vs Information Systems)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict\n",
    "from soc_fuzzy_match import fuzzy_match_soc_codes, generate_fuzzy_match_diagnostic\n",
    "import warnings\n",
    "from did_analysis import run_did_analysis\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION: MANUAL FOD \u2192 CIP4 MAPPINGS\n",
    "# =============================================================================\n",
    "# These are mappings not present in the crosswalk file or that need correction.\n",
    "# Each entry: dict with FOD, CIP4, CIP4_title, notes\n",
    "# To add more: just append to this list!\n",
    "\n",
    "MANUAL_MAPPINGS = [\n",
    "    {\n",
    "        'FOD': 6107,\n",
    "        'CIP4': '5138',\n",
    "        'CIP4_title': 'Registered Nursing/Nursing Administration/Nursing Research and Clinical Nursing',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 6107 missing mapping to CIP 5138 (490K students)'\n",
    "    },\n",
    "    {\n",
    "        'FOD': 3611,\n",
    "        'CIP4': '2615',\n",
    "        'CIP4_title': 'Neurobiology and Neurosciences',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 3611 not in original crosswalk'\n",
    "    },\n",
    "    {\n",
    "        'FOD': 5202,\n",
    "        'CIP4': '4228',\n",
    "        'CIP4_title': 'Clinical, Counseling and Applied Psychology',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5202 (Clinical Psychology) maps to CIP 4228'\n",
    "    },\n",
    "    {\n",
    "        'FOD': 5203,\n",
    "        'CIP4': '4228',\n",
    "        'CIP4_title': 'Clinical, Counseling and Applied Psychology',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5203 (Counseling Psychology) maps to CIP 4228'\n",
    "    },\n",
    "\n",
    "      {\n",
    "        'FOD': 5203,\n",
    "        'CIP4': '4228',\n",
    "        'CIP4_title': 'Clinical, Counseling and Applied Psychology',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5203 (Counseling Psychology) maps to CIP 4228'\n",
    "    },\n",
    "     {\n",
    "        'FOD': 5098,\n",
    "        'CIP4': '4099',\n",
    "        'CIP4_title': 'Physical Sciences, other',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5098 (Multi-disciplinary or General Science) maps to CIP 4099'\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(MANUAL_MAPPINGS)} manual FOD\u2192CIP4 mappings\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD FELTEN AIOE DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_felten_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load Felten et al. (2021) AIOE scores.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING FELTEN AIOE DATA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    felten = pd.read_excel(filepath, sheet_name='LM AIOE')\n",
    "    felten['soc_clean'] = felten['SOC Code'].str.replace('-', '').str.replace('.', '')\n",
    "    felten['AIOE'] = felten['Language Modeling AIOE'] \n",
    "    print(f\"\\nLoaded {len(felten)} occupations\")\n",
    "    print(f\"AIOE range: {felten['AIOE'].min():.2f} to {felten['AIOE'].max():.2f}\")\n",
    "\n",
    "    return felten\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: LOAD FOD TO 4-DIGIT CIP CROSSWALK\n",
    "# =============================================================================\n",
    "\n",
    "def load_fod_cip4_crosswalk(filepath: str, manual_mappings: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load FOD to 4-digit CIP mapping from crosswalk file.\n",
    "\n",
    "    The crosswalk has detailed 6-digit CIP codes (like 11.0701).\n",
    "    We extract 4-digit CIP:\n",
    "    - Family (2 digits): 11 = Computer Science\n",
    "    - Group (next 2 digits): 07 = Computer Science \n",
    "    - Combined: 1107 = Computer Science (4-digit)\n",
    "\n",
    "    Examples:\n",
    "    - 11.0000 \u2192 1100 (Computer Science, General)\n",
    "    - 11.0701 \u2192 1107 (Computer Science)\n",
    "    - 52.0201 \u2192 5202 (Business Administration)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to crosswalk Excel file\n",
    "    manual_mappings : list of dict\n",
    "        Additional manual mappings to append. Each dict should have keys:\n",
    "        'FOD', 'CIP4', 'CIP4_title', 'notes'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with columns ['FOD', 'CIP4', 'CIP4_title']\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING FOD TO 4-DIGIT CIP CROSSWALK\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Read from \"CIP code by HHES code\" sheet\n",
    "    df = pd.read_excel(filepath, sheet_name='CIP code by HHES code', skiprows=1)\n",
    "\n",
    "    # Extract FOD and CIP columns\n",
    "    crosswalk = df[['HHES Code', 'CIP \\nCode', 'CIP Title']].copy()\n",
    "    crosswalk.columns = ['FOD', 'CIP', 'CIP_title']\n",
    "    crosswalk = crosswalk.dropna(subset=['FOD', 'CIP'])\n",
    "\n",
    "    # Convert FOD to integer\n",
    "    crosswalk['FOD'] = crosswalk['FOD'].astype(int)\n",
    "\n",
    "    # Extract 4-digit CIP from 6-digit CIP code\n",
    "    # CIP format: XX.XXXX where first 2 are family, next 2 are group\n",
    "    # E.g., 11.0701 \u2192 1107\n",
    "    def extract_cip4(cip_6digit):\n",
    "        try:\n",
    "            cip_float = float(cip_6digit)\n",
    "            # Get integer part (family, 2 digits)\n",
    "            family = int(cip_float)  # e.g., 11\n",
    "            # Get first 2 decimal digits (group)\n",
    "            decimal_part = cip_float - family  # e.g., 0.0701\n",
    "            # Extract first 2 decimal digits\n",
    "            group = int(round(decimal_part * 10000)) // 100  # e.g., 07\n",
    "            # Combine to 4-digit code\n",
    "            cip4 = f\"{family:02d}{group:02d}\"  # e.g., \"1107\"\n",
    "            return cip4\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    crosswalk['CIP4'] = crosswalk['CIP'].apply(extract_cip4)\n",
    "    crosswalk = crosswalk.dropna(subset=['CIP4'])\n",
    "\n",
    "    # Create many-to-many mapping (each FOD can map to multiple CIP4s)\n",
    "    # Keep CIP4_title for the most general title per CIP4\n",
    "    fod_to_cip4_df = crosswalk.groupby(['FOD', 'CIP4']).agg({\n",
    "        'CIP_title': 'first'  # Take first title (they're usually the same for same CIP4)\n",
    "    }).reset_index()\n",
    "    fod_to_cip4_df.columns = ['FOD', 'CIP4', 'CIP4_title']\n",
    "\n",
    "    print(f\"\\nLoaded {len(fod_to_cip4_df)} FOD\u2192CIP4 mappings from crosswalk file\")\n",
    "    print(f\"  {fod_to_cip4_df['FOD'].nunique()} unique FODs\")\n",
    "    print(f\"  {fod_to_cip4_df['CIP4'].nunique()} unique CIP4 codes\")\n",
    "    print(f\"  Average {len(fod_to_cip4_df) / fod_to_cip4_df['FOD'].nunique():.1f} CIP4 codes per FOD\")\n",
    "\n",
    "    # Append manual mappings if provided\n",
    "    if manual_mappings:\n",
    "        manual_df = pd.DataFrame(manual_mappings)[['FOD', 'CIP4', 'CIP4_title']]\n",
    "        fod_to_cip4_df = pd.concat([fod_to_cip4_df, manual_df], ignore_index=True)\n",
    "        print(f\"\\n\u2713 Added {len(manual_mappings)} manual mappings\")\n",
    "        for mapping in manual_mappings:\n",
    "            print(f\"  FOD {mapping['FOD']} \u2192 CIP4 {mapping['CIP4']} ({mapping['CIP4_title']})\")\n",
    "\n",
    "    # Show sample mappings\n",
    "    print(\"\\nSample mappings:\")\n",
    "    sample_fods = sorted(fod_to_cip4_df['FOD'].unique())[:10]\n",
    "    for fod in sample_fods:\n",
    "        cips = fod_to_cip4_df[fod_to_cip4_df['FOD'] == fod]['CIP4'].tolist()\n",
    "        print(f\"  FOD {fod} \u2192 CIP4 {cips}\")\n",
    "\n",
    "    return fod_to_cip4_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2B: ADD EMPIRICAL ENROLLMENT WEIGHTS TO FOD\u2192CIP4 MAPPING\n",
    "# =============================================================================\n",
    "\n",
    "def add_empirical_weights_to_crosswalk(\n",
    "    fod_to_cip4: pd.DataFrame,\n",
    "    enrollment: pd.DataFrame,\n",
    "    base_year: int = 2019\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add empirical enrollment weights to FOD\u2192CIP4 mapping.\n",
    "\n",
    "    For each FOD that maps to multiple CIP4s, calculate weights based on\n",
    "    actual 2019 enrollment: weight_i = enrollment_i / sum(enrollment for all CIP4s that FOD maps to)\n",
    "\n",
    "    This creates a Bayesian update: P(CIP4 | FOD) \u221d enrollment(CIP4)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fod_to_cip4 : DataFrame with columns ['FOD', 'CIP4', 'CIP4_title']\n",
    "    enrollment : DataFrame with columns ['CIP4', 'year', 'enrollment']\n",
    "    base_year : Year to use for calculating weights (default 2019)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with columns ['FOD', 'CIP4', 'CIP4_title', 'empirical_weight']\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ADDING EMPIRICAL ENROLLMENT WEIGHTS TO FOD\u2192CIP4 MAPPING\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Get base year enrollment\n",
    "    enroll_base = enrollment[enrollment['year'] == base_year][['CIP4', 'enrollment', 'CIP4_title']].copy()\n",
    "    print(f\"\\nUsing {base_year} enrollment as basis for weights\")\n",
    "    print(f\"  {len(enroll_base)} CIP4 codes have enrollment data\")\n",
    "\n",
    "    # Merge enrollment into crosswalk\n",
    "    crosswalk_with_enroll = fod_to_cip4.merge(\n",
    "        enroll_base[['CIP4', 'enrollment']],\n",
    "        on='CIP4',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # For CIP4s with no enrollment data, use a small value (1.0) as placeholder\n",
    "    crosswalk_with_enroll['enrollment'] = crosswalk_with_enroll['enrollment'].fillna(1.0)\n",
    "\n",
    "    # For each FOD, calculate weights as proportion of total enrollment\n",
    "    # weight_i = enrollment_i / sum_j(enrollment_j) for all j that FOD maps to\n",
    "    fod_totals = crosswalk_with_enroll.groupby('FOD')['enrollment'].transform('sum')\n",
    "    crosswalk_with_enroll['empirical_weight'] = crosswalk_with_enroll['enrollment'] / fod_totals\n",
    "\n",
    "    # Clean up\n",
    "    crosswalk_weighted = crosswalk_with_enroll[['FOD', 'CIP4', 'CIP4_title', 'empirical_weight']].copy()\n",
    "\n",
    "    # Report\n",
    "    print(f\"\\nCalculated empirical weights for {len(crosswalk_weighted)} FOD\u2192CIP4 mappings\")\n",
    "\n",
    "    # Show examples\n",
    "    print(\"\\nSample weighted mappings:\")\n",
    "    sample_fods = crosswalk_weighted['FOD'].unique()[:3]\n",
    "    for fod in sample_fods:\n",
    "        fod_mappings = crosswalk_weighted[crosswalk_weighted['FOD'] == fod]\n",
    "        print(f\"\\n  FOD {fod} maps to {len(fod_mappings)} CIP4 codes:\")\n",
    "        for _, row in fod_mappings.iterrows():\n",
    "            print(f\"    CIP4 {row['CIP4']}: weight = {row['empirical_weight']:.3f}\")\n",
    "\n",
    "    # Sanity check: weights should sum to 1.0 for each FOD\n",
    "    weight_sums = crosswalk_weighted.groupby('FOD')['empirical_weight'].sum()\n",
    "    if not np.allclose(weight_sums, 1.0):\n",
    "        print(f\"\\n\u26a0 WARNING: Some FOD weights don't sum to 1.0!\")\n",
    "        print(f\"  Min: {weight_sums.min():.6f}, Max: {weight_sums.max():.6f}\")\n",
    "    else:\n",
    "        print(f\"\\n\u2713 All FOD weights sum to 1.0\")\n",
    "\n",
    "    return crosswalk_weighted\n",
    "\n",
    "\n",
    "# STEP 3: LOAD AND PROCESS ACS PUMS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_filter_acs(\n",
    "    filepath: str,\n",
    "    age_min: int = 22,\n",
    "    age_max: int = 35\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and filter IPUMS ACS PUMS data.\n",
    "\n",
    "    Your ACS columns: DEGFIELDD, OCCSOC, PERWT, AGE, EDUC, YEAR, INCWAGE, EMPSTAT, WKSWORK1\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING ACS PUMS DATA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    acs = pd.read_csv(filepath)\n",
    "\n",
    "    print(f\"\\nInitial sample: {len(acs):,} observations\")\n",
    "\n",
    "    # Filter out missing, invalid, and zero FODs\n",
    "    acs_filtered = acs[\n",
    "        (acs['AGE'] >= age_min) & \n",
    "        (acs['AGE'] <= age_max) &\n",
    "        (acs['OCCSOC'].notna()) &\n",
    "        (acs['DEGFIELDD'].notna()) &\n",
    "        (acs['DEGFIELDD'] != 0)  # Exclude FOD = 0 (invalid/no field of degree)\n",
    "     &\n",
    "        (acs['SAMPLE'] == 201703)  # 2013-17 5-year ACS estimates\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"Filtered sample: {len(acs_filtered):,} observations\")\n",
    "    print(f\"  - Age {age_min}-{age_max}\")\n",
    "    print(f\"  - Valid occupation (OCCSOC) and field of degree (DEGFIELDD)\")\n",
    "\n",
    "    # Clean SOC codes\n",
    "    acs_filtered['soc_clean'] = acs_filtered['OCCSOC'].astype(str).str.replace('-', '').str.replace('.', '')\n",
    "\n",
    "    print(f\"\\nUnique DEGFIELDD codes: {acs_filtered['DEGFIELDD'].nunique()}\")\n",
    "    print(f\"Unique OCCSOC codes: {acs_filtered['OCCSOC'].nunique()}\")\n",
    "\n",
    "    return acs_filtered\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3B: CALCULATE CIP4-LEVEL WAGES FROM ACS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_cip4_wages(\n",
    "    acs: pd.DataFrame,\n",
    "    fod_to_cip4: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate mean wages by 4-digit CIP code from ACS data.\n",
    "\n",
    "    Uses same FOD\u2192CIP4 mapping approach as AI exposure calculation.\n",
    "    Filters to employed, full-time workers (EMPSTAT==1, WKSWORK1>=48).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    acs : ACS PUMS data (already age-filtered)\n",
    "    fod_to_cip4 : FOD\u2192CIP4 crosswalk with empirical_weight\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with CIP4-level wage statistics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"CALCULATING 4-DIGIT CIP-LEVEL WAGES FROM ACS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nStarting with {len(acs):,} ACS observations\")\n",
    "\n",
    "    # Check required columns exist\n",
    "    required_cols = ['EMPSTAT', 'WKSWORK2', 'INCWAGE', 'DEGFIELDD', 'PERWT']\n",
    "    missing_cols = [col for col in required_cols if col not in acs.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"\u274c ERROR: Missing required columns in ACS data: {missing_cols}\")\n",
    "\n",
    "    # Filter to employed, full-time workers\n",
    "    # EMPSTAT == 1: Employed\n",
    "    # WKSWORK2 in [5,6] (48-52 weeks): Worked at least 48 weeks\n",
    "    acs_employed = acs[\n",
    "        (acs['EMPSTAT'] == 1) &\n",
    "        (acs['WKSWORK2'].isin([5, 6])) &\n",
    "        (acs['INCWAGE'].notna()) &\n",
    "        (acs['INCWAGE'] > 0)  # Exclude zero or negative wages\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"Filtered to employed full-time workers: {len(acs_employed):,} observations\")\n",
    "    print(f\"  - EMPSTAT == 1 (Employed)\")\n",
    "    print(f\"  - WKSWORK2 in [5,6] (48-52 weeks) (Worked \u226548 weeks)\")\n",
    "    print(f\"  - Valid positive INCWAGE\")\n",
    "\n",
    "    # Map FOD to CIP4 using crosswalk\n",
    "    acs_with_cip = acs_employed.merge(\n",
    "        fod_to_cip4,\n",
    "        left_on='DEGFIELDD',\n",
    "        right_on='FOD',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    print(f\"\\nMapped to CIP4 codes: {len(acs_with_cip):,} observations\")\n",
    "    print(f\"  {acs_with_cip['CIP4'].nunique()} unique CIP4 codes\")\n",
    "\n",
    "    # Calculate split weights (PERWT \u00d7 empirical_weight)\n",
    "    acs_with_cip['weight_split'] = acs_with_cip['PERWT'] * acs_with_cip['empirical_weight']\n",
    "\n",
    "    # Calculate weighted average wage by CIP4\n",
    "    cip_wages = acs_with_cip.groupby('CIP4', as_index=False).apply(\n",
    "        lambda x: pd.Series({\n",
    "            'mean_wage_2019': np.average(x['INCWAGE'], weights=x['weight_split']),\n",
    "            'median_wage_2019': np.percentile(x['INCWAGE'], 50),  # Unweighted median\n",
    "            'n_wage_obs': len(x),\n",
    "            'n_wage_weighted': x['weight_split'].sum(),\n",
    "            'log_mean_wage_2019': np.log(np.average(x['INCWAGE'], weights=x['weight_split'])),\n",
    "            'CIP4_title': x['CIP4_title'].iloc[0] if 'CIP4_title' in x.columns else ''\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    print(f\"\\nCalculated wages for {len(cip_wages)} 4-digit CIP codes\")\n",
    "    print(\"\\nWage Distribution:\")\n",
    "    print(cip_wages['mean_wage_2019'].describe())\n",
    "\n",
    "    # Check for DEGFIELDDs that couldn't be mapped\n",
    "    unmapped_fods = set(acs_employed['DEGFIELDD'].unique()) - set(fod_to_cip4['FOD'].unique())\n",
    "    if unmapped_fods:\n",
    "        unmapped_count = acs_employed[acs_employed['DEGFIELDD'].isin(unmapped_fods)]['PERWT'].sum()\n",
    "        total_count = acs_employed['PERWT'].sum()\n",
    "        print(f\"\\n\u26a0 WARNING: {len(unmapped_fods)} DEGFIELDD codes could not be mapped to CIP4:\")\n",
    "        print(f\"  FODs: {sorted(list(unmapped_fods))[:20]}\")\n",
    "        print(f\"  Represents {unmapped_count:,.0f} / {total_count:,.0f} employed workers ({unmapped_count/total_count*100:.1f}%)\")\n",
    "\n",
    "    # Show top and bottom wage majors\n",
    "    print(\"\\n\\nTop 20 highest-wage majors (4-digit CIP):\")\n",
    "    top20 = cip_wages.nlargest(20, 'mean_wage_2019')[['CIP4', 'CIP4_title', 'mean_wage_2019', 'n_wage_obs']]\n",
    "    for _, row in top20.iterrows():\n",
    "        print(f\"  {row['CIP4']:<8} ${row['mean_wage_2019']:>8,.0f}  {row['CIP4_title'][:50]:<50} (n={row['n_wage_obs']:,})\")\n",
    "\n",
    "    print(\"\\n\\nBottom 20 lowest-wage majors (4-digit CIP):\")\n",
    "    bottom20 = cip_wages.nsmallest(20, 'mean_wage_2019')[['CIP4', 'CIP4_title', 'mean_wage_2019', 'n_wage_obs']]\n",
    "    for _, row in bottom20.iterrows():\n",
    "        print(f\"  {row['CIP4']:<8} ${row['mean_wage_2019']:>8,.0f}  {row['CIP4_title'][:50]:<50} (n={row['n_wage_obs']:,})\")\n",
    "\n",
    "    return cip_wages\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: MAP FOD TO 4-DIGIT CIP AND MERGE WITH EXPOSURE\n",
    "# =============================================================================\n",
    "\n",
    "def process_acs_with_exposure(\n",
    "    acs: pd.DataFrame,\n",
    "    felten: pd.DataFrame,\n",
    "    fod_to_cip4: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map ACS FOD codes to 4-digit CIP using many-to-many relationship.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MAPPING FOD TO 4-DIGIT CIP AND MERGING AI EXPOSURE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Map FOD to CIP4\n",
    "    # Map FOD to CIP4 using many-to-many relationship\n",
    "    # Each ACS observation can contribute to multiple CIP4 codes\n",
    "    acs_with_cip = acs.merge(\n",
    "        fod_to_cip4,\n",
    "        left_on='DEGFIELDD',\n",
    "        right_on='FOD',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Report mapping success\n",
    "    n_original = len(acs)\n",
    "    n_after_mapping = len(acs_with_cip)\n",
    "    n_unique_people = acs_with_cip['DEGFIELDD'].nunique() if 'DEGFIELDD' in acs_with_cip.columns else len(acs_with_cip)\n",
    "\n",
    "    # Use empirical weights from crosswalk (already calculated based on 2019 enrollment)\n",
    "    # Each ACS person contributes weight_split = PERWT * empirical_weight to each CIP4\n",
    "    acs_with_cip['weight_split'] = acs_with_cip['PERWT'] * acs_with_cip['empirical_weight']\n",
    "\n",
    "    avg_cips = len(acs_with_cip) / len(acs)\n",
    "    print(f\"  Each ACS person contributes to avg {avg_cips:.1f} CIP4 codes (weighted by 2019 enrollment)\")\n",
    "\n",
    "\n",
    "    print(f\"\\nMapped {n_unique_people:,} ACS observations to {len(acs_with_cip):,} CIP4 mappings\")\n",
    "    print(f\"  (Average {n_after_mapping/n_unique_people if n_unique_people > 0 else 0:.1f} CIP4 codes per person)\")\n",
    "\n",
    "    # Check unmapped FODs\n",
    "    if len(acs_with_cip) < len(acs):\n",
    "        unmapped = acs[~acs['DEGFIELDD'].isin(fod_to_cip4['FOD'])]\n",
    "        unmapped_fods = unmapped['DEGFIELDD'].value_counts().head(10)\n",
    "        print(\"\\nTop 10 unmapped FOD codes:\")\n",
    "        print(unmapped_fods)\n",
    "\n",
    "\n",
    "    # Filter to successfully mapped\n",
    "\n",
    "    # Use fuzzy matching to handle masked SOC codes (XX/YY suffixes)\n",
    "    acs_with_exposure = fuzzy_match_soc_codes(acs_with_cip, felten)\n",
    "\n",
    "\n",
    "    # Drop observations with missing AIOE (do NOT impute with mean)\n",
    "    n_missing = acs_with_exposure['AIOE'].isna().sum()\n",
    "    if n_missing > 0:\n",
    "        print(f\"\\n\u26a0 Dropping {n_missing:,} observations with missing AIOE scores\")\n",
    "        acs_with_exposure = acs_with_exposure[acs_with_exposure['AIOE'].notna()].copy()\n",
    "\n",
    "    print(f\"\\nFinal sample: {len(acs_with_exposure):,} observations\")\n",
    "    print(f\"Unique 4-digit CIP codes: {acs_with_exposure['CIP4'].nunique()}\")\n",
    "\n",
    "    return acs_with_exposure\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: CALCULATE 4-DIGIT CIP-LEVEL AI EXPOSURE\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_cip4_exposure(\n",
    "    acs: pd.DataFrame,\n",
    "    weight_var: str = 'weight_split'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate weighted average AI exposure by 4-digit CIP code.\n",
    "\n",
    "    For each CIP4: AI_exposure = \u03a3 [P(occupation|CIP4) \u00d7 AIOE(occupation)]\n",
    "    where P(occupation|CIP4) is weighted by split weights (PERWT \u00d7 empirical_weight).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CALCULATING 4-DIGIT CIP-LEVEL AI EXPOSURE SCORES\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Define function to calculate exposure, handling zero-weight cases\n",
    "    def calculate_cip_exposure_stats(x):\n",
    "        \"\"\"Calculate AI exposure stats for a CIP4 group, handling zero-weight cases.\"\"\"\n",
    "        total_weight = x[weight_var].sum()\n",
    "\n",
    "        # Skip groups with zero or near-zero total weight\n",
    "        if pd.isna(total_weight) or total_weight < 1e-10:\n",
    "            return pd.Series({\n",
    "                'ai_exposure_score': np.nan,\n",
    "                'n_obs': len(x),\n",
    "                'n_weighted': 0.0,\n",
    "                'min_exposure': np.nan,\n",
    "                'max_exposure': np.nan,\n",
    "                'std_exposure': np.nan,\n",
    "                'CIP4_title': x['CIP4_title'].iloc[0] if 'CIP4_title' in x.columns else ''\n",
    "            })\n",
    "\n",
    "        mean_exposure = np.average(x['AIOE'], weights=x[weight_var])\n",
    "\n",
    "        return pd.Series({\n",
    "            'ai_exposure_score': mean_exposure,\n",
    "            'n_obs': len(x),\n",
    "            'n_weighted': total_weight,\n",
    "            'min_exposure': x['AIOE'].min(),\n",
    "            'max_exposure': x['AIOE'].max(),\n",
    "            'std_exposure': np.sqrt(np.average((x['AIOE'] - mean_exposure)**2, weights=x[weight_var])),\n",
    "            'CIP4_title': x['CIP4_title'].iloc[0] if 'CIP4_title' in x.columns else ''\n",
    "        })\n",
    "\n",
    "    # Calculate weighted average by CIP4\n",
    "    cip_exposure = acs.groupby('CIP4', as_index=False).apply(\n",
    "        calculate_cip_exposure_stats\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Check for zero-weight groups\n",
    "    zero_weight_groups = cip_exposure[cip_exposure['n_weighted'] < 1e-10]\n",
    "    if len(zero_weight_groups) > 0:\n",
    "        print(f\"\\n\u26a0 WARNING: {len(zero_weight_groups)} CIP4 codes have zero total weight (exposure set to NaN):\")\n",
    "        for _, row in zero_weight_groups.head(10).iterrows():\n",
    "            print(f\"  CIP4 {row['CIP4']} ({row['CIP4_title'][:40]}): n={row['n_obs']}\")\n",
    "\n",
    "    print(f\"\\nCalculated AI exposure for {len(cip_exposure)} 4-digit CIP codes\")\n",
    "    print(f\"  {cip_exposure['ai_exposure_score'].notna().sum()} with valid exposure data\")\n",
    "    print(\"\\nAI Exposure Score Distribution:\")\n",
    "    print(cip_exposure['ai_exposure_score'].describe())\n",
    "\n",
    "    # Show top and bottom CIPs\n",
    "    print(\"\\n\\nTop 20 most AI-exposed majors (4-digit CIP):\")\n",
    "    top20 = cip_exposure.nlargest(20, 'ai_exposure_score')[['CIP4', 'CIP4_title', 'ai_exposure_score', 'n_obs']]\n",
    "    print(top20.to_string(index=False))\n",
    "\n",
    "    print(\"\\n\\nBottom 20 least AI-exposed majors (4-digit CIP):\")\n",
    "    bottom20 = cip_exposure.nsmallest(20, 'ai_exposure_score')[['CIP4', 'CIP4_title', 'ai_exposure_score', 'n_obs']]\n",
    "    print(bottom20.to_string(index=False))\n",
    "\n",
    "    return cip_exposure\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: LOAD AND COMBINE ENROLLMENT DATA (2019-2025)\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_combine_enrollment_data(\n",
    "    filepath_2024: str, \n",
    "    filepath_2025: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine enrollment data from two sources with proper header handling.\n",
    "\n",
    "    2024 file: Major Field (4-year, Undergrad) sheet, years 2019-2024\n",
    "    2025 file: CIP Group Enrollment sheet, years 2020-2025 (filter to Undergraduate 4-year)\n",
    "\n",
    "    Returns combined dataset with 4-digit CIP codes (2019-2025), including CIP4_title.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING AND COMBINING ENROLLMENT DATA (2019-2025)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ===== LOAD 2024 FILE =====\n",
    "    print(\"\\nLoading 2019-2024 data from CTEESpring2024-Appendix.xlsx...\")\n",
    "    df_2024 = pd.read_excel(\n",
    "        filepath_2024, \n",
    "        sheet_name='Major Field (4-year, Undergrad)',\n",
    "        header=2  # Row 2 has the actual column headers\n",
    "    )\n",
    "    print(f\"  Loaded {len(df_2024)} rows\")\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    df_2024 = df_2024.rename(columns={\n",
    "        'Major Field Family (2-digit CIP)': 'CIP2',\n",
    "        'Major Field Family (2-digit) Title': 'CIP2_title',\n",
    "        'Major Field Group (4-digit CIP)': 'CIP4',\n",
    "        'Major Field Group (4-digit) Title': 'CIP4_title'\n",
    "    })\n",
    "\n",
    "    # Get enrollment columns (years 2019-2024)\n",
    "    years_2024 = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "    enrollment_cols = [col for col in df_2024.columns if 'Enrollment' in str(col) and '% Change' not in str(col)]\n",
    "    print(f\"  Found {len(enrollment_cols)} enrollment columns for years 2019-2024\")\n",
    "\n",
    "    # Reshape to long format\n",
    "    data_2024 = []\n",
    "    for idx, row in df_2024.iterrows():\n",
    "        cip4 = row['CIP4']\n",
    "        cip4_title = row['CIP4_title']\n",
    "        if pd.isna(cip4) or cip4 == 'Total':\n",
    "            continue\n",
    "        for year, col in zip(years_2024, enrollment_cols):\n",
    "            enrollment = row[col]\n",
    "            if pd.notna(enrollment) and enrollment != '*':\n",
    "                data_2024.append({\n",
    "                    'CIP4': str(cip4)[:4] if pd.notna(cip4) and str(cip4) != 'Total' else None,\n",
    "                    'CIP4_title': cip4_title,\n",
    "                    'year': year,\n",
    "                    'enrollment': float(enrollment)\n",
    "                })\n",
    "\n",
    "    df_2024_long = pd.DataFrame(data_2024)\n",
    "    print(f\"  Reshaped to {len(df_2024_long)} observations\")\n",
    "\n",
    "    # ===== LOAD 2025 FILE =====\n",
    "    print(\"\\nLoading 2020-2025 data from CTEESpring2025-DataAppendix.xlsx...\")\n",
    "    df_2025 = pd.read_excel(\n",
    "        filepath_2025,\n",
    "        sheet_name='CIP Group Enrollment',\n",
    "        header=2  # Row 2 has the actual column headers\n",
    "    )\n",
    "    print(f\"  Loaded {len(df_2025)} rows\")\n",
    "\n",
    "    # Filter to Undergraduate 4-year only\n",
    "    df_2025 = df_2025[df_2025['Award Level and Institution Type'] == 'Undergraduate 4-year'].copy()\n",
    "    print(f\"  Filtered to {len(df_2025)} Undergraduate 4-year rows\")\n",
    "\n",
    "    # Rename columns\n",
    "    df_2025 = df_2025.rename(columns={\n",
    "        'Major Field Family \\n(2-digit CIP)': 'CIP2',\n",
    "        'Major Field Family \\n(2-digit CIP) Title': 'CIP2_title',\n",
    "        'Major Field Group \\n(4-digit CIP)': 'CIP4',\n",
    "        'Major Field Group \\n(4-digit CIP) Title': 'CIP4_title'\n",
    "    })\n",
    "\n",
    "    # Get enrollment columns (years 2020-2025)\n",
    "    # The enrollment columns alternate: Enrollment, % Change, Enrollment, % Change...\n",
    "    # Columns 5, 6, 8, 10, 12, 14 correspond to years 2020-2025\n",
    "    years_2025 = [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "    enrollment_col_indices = [5, 6, 8, 10, 12, 14]\n",
    "\n",
    "    # Reshape to long format\n",
    "    data_2025 = []\n",
    "    for idx, row in df_2025.iterrows():\n",
    "        cip4 = row['CIP4']\n",
    "        cip4_title = row['CIP4_title']\n",
    "        if pd.isna(cip4) or cip4 == 'Total':\n",
    "            continue\n",
    "        for year, col_idx in zip(years_2025, enrollment_col_indices):\n",
    "            enrollment = row.iloc[col_idx]\n",
    "            if pd.notna(enrollment) and enrollment != '*':\n",
    "                data_2025.append({\n",
    "                    'CIP4': str(cip4)[:4] if pd.notna(cip4) and str(cip4) != 'Total' else None,\n",
    "                    'CIP4_title': cip4_title,\n",
    "                    'year': year,\n",
    "                    'enrollment': float(enrollment)\n",
    "                })\n",
    "\n",
    "    df_2025_long = pd.DataFrame(data_2025)\n",
    "    print(f\"  Reshaped to {len(df_2025_long)} observations\")\n",
    "\n",
    "    # ===== COMBINE DATASETS =====\n",
    "    print(\"\\nCombining datasets...\")\n",
    "\n",
    "    # For overlapping years (2020-2024), use 2025 file data (more recent)\n",
    "    df_2024_unique = df_2024_long[df_2024_long['year'] == 2019].copy()\n",
    "\n",
    "    enrollment = pd.concat([df_2024_unique, df_2025_long], axis=0, ignore_index=True)\n",
    "    enrollment = enrollment.sort_values(['CIP4', 'year']).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n\u2713 Combined dataset: {len(enrollment)} observations\")\n",
    "    print(f\"  Years: {sorted(enrollment['year'].unique())}\")\n",
    "    print(f\"  Unique 4-digit CIP codes: {enrollment['CIP4'].nunique()}\")\n",
    "\n",
    "    # Summary stats\n",
    "    print(\"\\nTotal enrollment by year:\")\n",
    "    yearly_enrollment = enrollment.groupby('year')['enrollment'].sum()\n",
    "    for year, total in yearly_enrollment.items():\n",
    "        print(f\"  {year}: {total:,.0f}\")\n",
    "\n",
    "    return enrollment\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: MERGE AND FINALIZE\n",
    "# =============================================================================\n",
    "\n",
    "def merge_enrollment_exposure_wages(\n",
    "    enrollment: pd.DataFrame,\n",
    "    cip_exposure: pd.DataFrame,\n",
    "    cip_wages: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge enrollment data with AI exposure scores and wage data.\n",
    "    Preserves CIP4_title from enrollment data (most complete).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MERGING ENROLLMENT WITH AI EXPOSURE AND WAGES\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Normalize CIP4 codes to match (all as zero-padded 4-char strings)\n",
    "    enrollment['CIP4'] = enrollment['CIP4'].astype(str).str.zfill(4)\n",
    "    cip_exposure['CIP4'] = cip_exposure['CIP4'].astype(str).str.zfill(4)\n",
    "    cip_wages['CIP4'] = cip_wages['CIP4'].astype(str).str.zfill(4)\n",
    "\n",
    "    # First merge enrollment with exposure\n",
    "    df_final = enrollment.merge(\n",
    "        cip_exposure[['CIP4', 'ai_exposure_score', 'n_obs']],\n",
    "        on='CIP4',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Then merge with wages\n",
    "    df_final = df_final.merge(\n",
    "        cip_wages[['CIP4', 'mean_wage_2019', 'median_wage_2019', 'log_mean_wage_2019', 'n_wage_obs']],\n",
    "        on='CIP4',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Report merge success\n",
    "    n_matched_exposure = df_final['ai_exposure_score'].notna().sum()\n",
    "    n_matched_wages = df_final['mean_wage_2019'].notna().sum()\n",
    "    pct_matched_exposure = 100 * n_matched_exposure / len(df_final)\n",
    "    pct_matched_wages = 100 * n_matched_wages / len(df_final)\n",
    "\n",
    "    print(f\"\\nMatched {n_matched_exposure}/{len(df_final)} to AI exposure ({pct_matched_exposure:.1f}%)\")\n",
    "    print(f\"Matched {n_matched_wages}/{len(df_final)} to wages ({pct_matched_wages:.1f}%)\")\n",
    "\n",
    "    # Check for CIP4s missing wage data\n",
    "    missing_wage_cips = df_final[df_final['mean_wage_2019'].isna()]['CIP4'].unique()\n",
    "    if len(missing_wage_cips) > 0:\n",
    "        print(f\"\\n\u26a0 WARNING: {len(missing_wage_cips)} CIP4 codes missing wage data\")\n",
    "        print(f\"  First 20: {sorted(list(missing_wage_cips))[:20]}\")\n",
    "\n",
    "    # Create treatment variables\n",
    "    median_exposure = df_final['ai_exposure_score'].median()\n",
    "    df_final['high_ai_exposure'] = (\n",
    "        df_final['ai_exposure_score'] > median_exposure\n",
    "    ).astype(int)\n",
    "\n",
    "    # Standardized exposure\n",
    "    df_final['ai_exposure_std'] = (\n",
    "        (df_final['ai_exposure_score'] - df_final['ai_exposure_score'].mean()) /\n",
    "        df_final['ai_exposure_score'].std()\n",
    "    )\n",
    "\n",
    "    # Terciles (with error handling for insufficient unique values)\n",
    "    try:\n",
    "        df_final['ai_exposure_tercile'] = pd.qcut(\n",
    "            df_final['ai_exposure_score'],\n",
    "            q=3,\n",
    "            labels=['Low', 'Medium', 'High'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        # If qcut fails (e.g., too many NaNs or duplicates), use simple cut\n",
    "        print(f\"\u26a0 Could not create terciles: {e}\")\n",
    "        print(\"  Using quartile-based cut instead\")\n",
    "        df_final['ai_exposure_tercile'] = pd.cut(\n",
    "            df_final['ai_exposure_score'],\n",
    "            bins=3,\n",
    "            labels=['Low', 'Medium', 'High']\n",
    "        )\n",
    "\n",
    "    # Create wage quartiles for DiD controls\n",
    "    if df_final['mean_wage_2019'].notna().sum() > 0:\n",
    "        df_final['wage_quartile'] = pd.qcut(\n",
    "            df_final['mean_wage_2019'],\n",
    "            q=4,\n",
    "            labels=['Q1', 'Q2', 'Q3', 'Q4'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        print(f\"\\n\u2713 Created wage quartiles for {df_final['wage_quartile'].notna().sum()} observations\")\n",
    "\n",
    "    # Create log enrollment\n",
    "    df_final['log_enrollment'] = np.log(df_final['enrollment'] + 1)\n",
    "\n",
    "    print(\"\\n\\nFinal dataset:\")\n",
    "    print(df_final.head(20))\n",
    "    print(f\"\\nShape: {df_final.shape}\")\n",
    "    print(f\"Columns: {list(df_final.columns)}\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_descriptive_plots(df: pd.DataFrame, output_path: str = 'enrollment_trends_4digit.png'):\n",
    "    \"\"\"\n",
    "    Create descriptive visualizations for 4-digit CIP analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # 1. Enrollment trends by AI exposure group\n",
    "    ax1 = axes[0, 0]\n",
    "    trend_data = df.groupby(['year', 'high_ai_exposure'])['enrollment'].sum().reset_index()\n",
    "    # Normalize to 2019 (show as % of 2019 enrollment)\n",
    "    trend_2019 = trend_data[trend_data['year'] == 2019].set_index('high_ai_exposure')['enrollment']\n",
    "    trend_data['enrollment_pct_2019'] = trend_data.apply(\n",
    "        lambda row: (row['enrollment'] / trend_2019[row['high_ai_exposure']]) * 100\n",
    "            if row['high_ai_exposure'] in trend_2019.index else 100,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    for group in [0, 1]:\n",
    "        data = trend_data[trend_data['high_ai_exposure'] == group]\n",
    "        label = 'High AI Exposure' if group else 'Low AI Exposure'\n",
    "        ax1.plot(data['year'], data['enrollment_pct_2019'], marker='o', label=label, linewidth=2)\n",
    "    ax1.set_xlabel('Year', fontsize=12)\n",
    "    ax1.set_ylabel('Enrollment (% of 2019)', fontsize=12)\n",
    "    ax1.set_title('Enrollment Trends by AI Exposure (4-digit CIP)', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.axvline(2022.5, color='red', linestyle='--', alpha=0.5, label='ChatGPT Launch')\n",
    "\n",
    "    # 2. Distribution of AI exposure\n",
    "    ax2 = axes[0, 1]\n",
    "    cip_scores = df.groupby('CIP4')['ai_exposure_score'].first()\n",
    "    ax2.hist(cip_scores, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax2.axvline(cip_scores.median(), color='red', linestyle='--', linewidth=2, label='Median')\n",
    "    ax2.set_xlabel('AI Exposure Score', fontsize=12)\n",
    "    ax2.set_ylabel('Number of 4-digit CIP Codes', fontsize=12)\n",
    "    ax2.set_title('Distribution of AI Exposure Across Majors', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    # 3. Scatter: enrollment growth vs exposure\n",
    "    ax3 = axes[1, 0]\n",
    "    first_year = df['year'].min()\n",
    "    last_year = df['year'].max()\n",
    "\n",
    "    growth_data = []\n",
    "    for cip in df['CIP4'].unique():\n",
    "        cip_data = df[df['CIP4'] == cip]\n",
    "        enroll_first = cip_data[cip_data['year'] == first_year]['enrollment'].values\n",
    "        enroll_last = cip_data[cip_data['year'] == last_year]['enrollment'].values\n",
    "        if len(enroll_first) > 0 and len(enroll_last) > 0 and enroll_first[0] > 0:\n",
    "            growth = (enroll_last[0] - enroll_first[0]) / enroll_first[0] * 100\n",
    "            exposure = cip_data['ai_exposure_score'].iloc[0] if len(cip_data) > 0 else None\n",
    "            if exposure is not None and pd.notna(exposure):\n",
    "                growth_data.append({'CIP4': cip, 'growth_rate': growth, 'ai_exposure': exposure})\n",
    "\n",
    "    growth_df = pd.DataFrame(growth_data)\n",
    "    if len(growth_df) > 0:\n",
    "        ax3.scatter(growth_df['ai_exposure'], growth_df['growth_rate'], alpha=0.6, s=30)\n",
    "        ax3.set_xlabel('AI Exposure Score', fontsize=12)\n",
    "        ax3.set_ylabel(f'Enrollment Growth Rate ({first_year}-{last_year}, %)', fontsize=12)\n",
    "        ax3.set_title('Growth Rate vs AI Exposure', fontsize=14, fontweight='bold')\n",
    "        ax3.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax3.grid(alpha=0.3)\n",
    "\n",
    "        # Add correlation\n",
    "        corr = growth_df[['ai_exposure', 'growth_rate']].corr().iloc[0, 1]\n",
    "        ax3.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                transform=ax3.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # 4. Enrollment by tercile over time\n",
    "    ax4 = axes[1, 1]\n",
    "    tercile_data = df.groupby(['year', 'ai_exposure_tercile'])['enrollment'].sum().reset_index()\n",
    "    # Normalize to 2019\n",
    "    tercile_2019 = tercile_data[tercile_data['year'] == 2019].set_index('ai_exposure_tercile')['enrollment']\n",
    "    tercile_data['enrollment_pct_2019'] = tercile_data.apply(\n",
    "        lambda row: (row['enrollment'] / tercile_2019[row['ai_exposure_tercile']]) * 100\n",
    "            if row['ai_exposure_tercile'] in tercile_2019.index else 100,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    for tercile in ['Low', 'Medium', 'High']:\n",
    "        data = tercile_data[tercile_data['ai_exposure_tercile'] == tercile]\n",
    "        if len(data) > 0:\n",
    "            ax4.plot(data['year'], data['enrollment_pct_2019'], marker='o', label=f'{tercile} Exposure', linewidth=2)\n",
    "    ax4.set_xlabel('Year', fontsize=12)\n",
    "    ax4.set_ylabel('Enrollment (% of 2019)', fontsize=12)\n",
    "    ax4.set_title('Enrollment by AI Exposure Tercile', fontsize=14, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(alpha=0.3)\n",
    "    ax4.axvline(2022.5, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved plots to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8B: TERCILE DEEP-DIVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_tercile_deepdive_plots(df: pd.DataFrame, output_path: str = 'enrollment_tercile_deepdive.png'):\n",
    "    \"\"\"\n",
    "    Create detailed enrollment trend plots for top 5 majors within each AI exposure tercile.\n",
    "\n",
    "    For each tercile (Low/Medium/High), shows:\n",
    "    - Top 5 CIP4 codes by 2019 enrollment\n",
    "    - Enrollment trends 2019-2025 (normalized to 2019 = 100%)\n",
    "    - CIP4 labels with titles\n",
    "    - 2025: Actual enrollment number labeled\n",
    "    - 2023: AI exposure score labeled\n",
    "    - % coverage: what fraction of tercile enrollment these top 5 represent\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING TERCILE DEEP-DIVE PLOTS (TOP 5)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Filter to rows with valid tercile assignment\n",
    "    df_valid = df[df['ai_exposure_tercile'].notna()].copy()\n",
    "\n",
    "    # Create figure with 3 subplots (1 row x 3 cols)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "    # Color palette for 5 lines\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 5))\n",
    "\n",
    "    for idx, tercile in enumerate(['Low', 'Medium', 'High']):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Get data for this tercile\n",
    "        tercile_data = df_valid[df_valid['ai_exposure_tercile'] == tercile].copy()\n",
    "\n",
    "        if len(tercile_data) == 0:\n",
    "            print(f\"\u26a0 No data for {tercile} tercile, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Get 2019 baseline\n",
    "        tercile_2019 = tercile_data[tercile_data['year'] == 2019].copy()\n",
    "\n",
    "        # Get top 5 CIP4 by 2019 enrollment\n",
    "        top5_cip4s = tercile_2019.nlargest(5, 'enrollment')['CIP4'].values\n",
    "\n",
    "        # Calculate coverage\n",
    "        top5_enrollment = tercile_2019[tercile_2019['CIP4'].isin(top5_cip4s)]['enrollment'].sum()\n",
    "        total_enrollment = tercile_2019['enrollment'].sum()\n",
    "        coverage_pct = (top5_enrollment / total_enrollment * 100) if total_enrollment > 0 else 0\n",
    "\n",
    "        print(f\"\\n{tercile} Tercile:\")\n",
    "        print(f\"  Top 5 CIP4s: {list(top5_cip4s)}\")\n",
    "        print(f\"  Coverage: {coverage_pct:.1f}% of {tercile} tercile enrollment\")\n",
    "        print(f\"  2019 enrollment in top 5: {top5_enrollment:,.0f} / {total_enrollment:,.0f}\")\n",
    "\n",
    "        # For each of the top 5 CIP4s, plot enrollment trend\n",
    "        for i, cip4 in enumerate(top5_cip4s):\n",
    "            cip_data = tercile_data[tercile_data['CIP4'] == cip4].copy()\n",
    "\n",
    "            if len(cip_data) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get 2019 baseline for this CIP4\n",
    "            baseline_2019 = cip_data[cip_data['year'] == 2019]['enrollment'].values\n",
    "            if len(baseline_2019) == 0 or baseline_2019[0] == 0:\n",
    "                continue\n",
    "            baseline_2019 = baseline_2019[0]\n",
    "\n",
    "            # Normalize to 2019 = 100%\n",
    "            cip_data['enrollment_pct'] = (cip_data['enrollment'] / baseline_2019) * 100\n",
    "\n",
    "            # Get CIP4 title (truncate if too long)\n",
    "            cip4_title = cip_data['CIP4_title'].iloc[0] if len(cip_data) > 0 else ''\n",
    "            if len(cip4_title) > 30:\n",
    "                cip4_title = cip4_title[:27] + '...'\n",
    "\n",
    "            # Get AI exposure score\n",
    "            ai_exposure = cip_data['ai_exposure_score'].iloc[0] if len(cip_data) > 0 else None\n",
    "\n",
    "            # Plot\n",
    "            label = f\"{cip4}: {cip4_title}\"\n",
    "            ax.plot(cip_data['year'], cip_data['enrollment_pct'], \n",
    "                   marker='o', label=label, linewidth=2.5, color=colors[i], alpha=0.8, markersize=6)\n",
    "\n",
    "            # Add label for 2025 (actual enrollment)\n",
    "            data_2025 = cip_data[cip_data['year'] == 2025]\n",
    "            if len(data_2025) > 0:\n",
    "                enrollment_2025 = data_2025['enrollment'].values[0]\n",
    "                enrollment_pct_2025 = data_2025['enrollment_pct'].values[0]\n",
    "\n",
    "                # Smart vertical offset to avoid overlap\n",
    "                offset = (i - 2) * 8  # Spread labels vertically (-16, -8, 0, 8, 16)\n",
    "\n",
    "                ax.annotate(f'{enrollment_2025:,.0f}',\n",
    "                           xy=(2025, enrollment_pct_2025),\n",
    "                           xytext=(8, offset),\n",
    "                           textcoords='offset points',\n",
    "                           fontsize=8,\n",
    "                           color=colors[i],\n",
    "                           fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor=colors[i], alpha=0.8))\n",
    "\n",
    "            # Add label for 2023 (AI exposure score)\n",
    "            data_2023 = cip_data[cip_data['year'] == 2023]\n",
    "            if len(data_2023) > 0 and ai_exposure is not None:\n",
    "                enrollment_pct_2023 = data_2023['enrollment_pct'].values[0]\n",
    "\n",
    "                # Smart vertical offset to avoid overlap\n",
    "                offset_y = (i - 2) * 6  # Spread labels vertically\n",
    "\n",
    "                ax.annotate(f'AI: {ai_exposure:.3f}',\n",
    "                           xy=(2023, enrollment_pct_2023),\n",
    "                           xytext=(-35, offset_y),\n",
    "                           textcoords='offset points',\n",
    "                           fontsize=7,\n",
    "                           color=colors[i],\n",
    "                           style='italic',\n",
    "                           bbox=dict(boxstyle='round,pad=0.2', facecolor='lightyellow', edgecolor=colors[i], alpha=0.7))\n",
    "\n",
    "        # Styling\n",
    "        ax.set_xlabel('Year', fontsize=12)\n",
    "        ax.set_ylabel('Enrollment (% of 2019)', fontsize=12)\n",
    "        ax.set_title(f'{tercile} AI Exposure - Top 5 Majors', fontsize=14, fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.axvline(2022.5, color='red', linestyle='--', alpha=0.5, linewidth=1.5, label='ChatGPT Launch')\n",
    "        ax.axhline(100, color='gray', linestyle=':', alpha=0.5, linewidth=1)\n",
    "\n",
    "        # Add coverage annotation\n",
    "        annotation_text = f\"Top 5: {coverage_pct:.1f}% of tercile\\nN = {top5_enrollment:,.0f} (2019)\"\n",
    "        ax.text(0.02, 0.98, annotation_text, \n",
    "               transform=ax.transAxes, \n",
    "               verticalalignment='top',\n",
    "               fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "        # Legend - smaller font, outside plot\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0, -0.12), \n",
    "                 ncol=1, fontsize=9, framealpha=0.9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n\u2713 Saved tercile deep-dive plots to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# # STEP 9: DIAGNOSTIC REPORTING\n",
    "# =============================================================================\n",
    "\n",
    "def generate_diagnostic_report(\n",
    "    acs: pd.DataFrame,\n",
    "    fod_to_cip4: pd.DataFrame,\n",
    "    enrollment: pd.DataFrame\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate diagnostic report showing coverage gaps:\n",
    "    (i) ACS FOD codes not in crosswalk\n",
    "    (ii) CIP4 codes with enrollment but no FOD mapping\n",
    "    (iii) Top unmapped ACS FODs by weighted person-count\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(\"# DIAGNOSTIC REPORT: COVERAGE ANALYSIS\")\n",
    "    print(\"#\"*70)\n",
    "\n",
    "    # (i) ACS FOD codes not in crosswalk\n",
    "    acs_fods = set(acs['DEGFIELDD'].dropna().unique())\n",
    "    crosswalk_fods = set(fod_to_cip4['FOD'].unique())\n",
    "    missing_fods = acs_fods - crosswalk_fods\n",
    "\n",
    "    print(f\"\\n(i) ACS FOD codes NOT in crosswalk mapping:\")\n",
    "    print(f\"    Total: {len(missing_fods)} FOD codes\")\n",
    "    if len(missing_fods) > 0:\n",
    "        print(f\"    FODs: {sorted(list(missing_fods))[:20]}\")\n",
    "        # How many ACS observations do these represent?\n",
    "        missing_fod_count = acs[acs['DEGFIELDD'].isin(missing_fods)]['PERWT'].sum()\n",
    "        total_count = acs['PERWT'].sum()\n",
    "        print(f\"    Represents {missing_fod_count:,.0f} / {total_count:,.0f} ACS observations ({missing_fod_count/total_count*100:.1f}%)\")\n",
    "\n",
    "    # (ii) CIP codes with enrollment but no FOD mapping\n",
    "    enrollment_cips = set(enrollment['CIP4'].unique())\n",
    "    crosswalk_cips = set(fod_to_cip4['CIP4'].unique())\n",
    "    unmapped_cips = enrollment_cips - crosswalk_cips\n",
    "\n",
    "    print(f\"\\n(ii) CIP4 codes with enrollment but NOT mapped from any FOD:\")\n",
    "    print(f\"     Total: {len(unmapped_cips)} CIP4 codes\")\n",
    "    if len(unmapped_cips) > 0:\n",
    "        # Get enrollment counts for these\n",
    "        unmapped_enroll = enrollment[enrollment['CIP4'].isin(unmapped_cips)]\n",
    "        unmapped_2019 = unmapped_enroll[unmapped_enroll['year'] == 2019]['enrollment'].sum()\n",
    "        total_2019 = enrollment[enrollment['year'] == 2019]['enrollment'].sum()\n",
    "        print(f\"     CIP4s: {sorted(list(unmapped_cips))[:30]}\")\n",
    "        print(f\"     2019 enrollment: {unmapped_2019:,.0f} / {total_2019:,.0f} ({unmapped_2019/total_2019*100:.1f}%)\")\n",
    "        print(f\"\\n     Top 10 unmapped CIP4s by 2019 enrollment:\")\n",
    "        top_unmapped = unmapped_enroll[unmapped_enroll['year'] == 2019].nlargest(10, 'enrollment')[['CIP4', 'CIP4_title', 'enrollment']]\n",
    "        for _, row in top_unmapped.iterrows():\n",
    "            print(f\"       CIP4 {row['CIP4']} ({row['CIP4_title']}): {row['enrollment']:,.0f} students\")\n",
    "\n",
    "    # (iii) NEW: Top unmapped ACS FODs by weighted person-count\n",
    "    print(f\"\\n(iii) Top 20 unmapped ACS FOD codes by weighted person-count:\")\n",
    "    if len(missing_fods) > 0:\n",
    "        unmapped_acs = acs[acs['DEGFIELDD'].isin(missing_fods)]\n",
    "        top_unmapped_fods = unmapped_acs.groupby('DEGFIELDD')['PERWT'].sum().sort_values(ascending=False).head(20)\n",
    "        print(f\"\\n     {'FOD':<8} {'Weighted Count':>15} {'% of Total':>10}\")\n",
    "        print(f\"     {'-'*8} {'-'*15} {'-'*10}\")\n",
    "        for fod, count in top_unmapped_fods.items():\n",
    "            pct = count / total_count * 100\n",
    "            print(f\"     {int(fod):<8} {count:>15,.0f} {pct:>9.2f}%\")\n",
    "    else:\n",
    "        print(\"     All ACS FODs are mapped!\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 10: DROP EXTREME GROWTH OUTLIERS\n",
    "# =============================================================================\n",
    "\n",
    "def drop_growth_outliers(df: pd.DataFrame, percentile: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop majors in the top and bottom percentile of 2019-2025 enrollment growth.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with enrollment data\n",
    "    percentile : float, percentile threshold (e.g., 1.0 for top/bottom 1%)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with outliers removed\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"DROPPING EXTREME GROWTH OUTLIERS (Top/Bottom {percentile}%)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Calculate 2019-2025 growth rate for each CIP4\n",
    "    growth_data = []\n",
    "    for cip4 in df['CIP4'].unique():\n",
    "        cip_data = df[df['CIP4'] == cip4].copy()\n",
    "\n",
    "        enroll_2019 = cip_data[cip_data['year'] == 2019]['enrollment'].values\n",
    "        enroll_2025 = cip_data[cip_data['year'] == 2025]['enrollment'].values\n",
    "\n",
    "        if len(enroll_2019) > 0 and len(enroll_2025) > 0 and enroll_2019[0] > 0:\n",
    "            growth_rate = (enroll_2025[0] - enroll_2019[0]) / enroll_2019[0] * 100\n",
    "            cip4_title = cip_data['CIP4_title'].iloc[0] if len(cip_data) > 0 else ''\n",
    "            ai_exposure = cip_data['ai_exposure_score'].iloc[0] if len(cip_data) > 0 else None\n",
    "\n",
    "            growth_data.append({\n",
    "                'CIP4': cip4,\n",
    "                'CIP4_title': cip4_title,\n",
    "                'ai_exposure_score': ai_exposure,\n",
    "                'enrollment_2019': enroll_2019[0],\n",
    "                'enrollment_2025': enroll_2025[0],\n",
    "                'growth_rate': growth_rate\n",
    "            })\n",
    "\n",
    "    growth_df = pd.DataFrame(growth_data)\n",
    "\n",
    "    # Calculate percentile thresholds\n",
    "    top_threshold = np.percentile(growth_df['growth_rate'].dropna(), 100 - percentile)\n",
    "    bottom_threshold = np.percentile(growth_df['growth_rate'].dropna(), percentile)\n",
    "\n",
    "    print(f\"\\nGrowth rate percentiles:\")\n",
    "    print(f\"  Bottom {percentile}%: {bottom_threshold:.2f}%\")\n",
    "    print(f\"  Top {percentile}%: {top_threshold:.2f}%\")\n",
    "\n",
    "    # Identify outliers\n",
    "    top_outliers = growth_df[growth_df['growth_rate'] >= top_threshold].copy()\n",
    "    bottom_outliers = growth_df[growth_df['growth_rate'] <= bottom_threshold].copy()\n",
    "\n",
    "    # Print top outliers\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TOP {percentile}% GROWTH OUTLIERS (>= {top_threshold:.2f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n{'CIP4':<8} {'Growth':<10} {'2019':<12} {'2025':<12} {'AI Exp':<8} {'Title':<50}\")\n",
    "    print(f\"{'-'*8} {'-'*10} {'-'*12} {'-'*12} {'-'*8} {'-'*50}\")\n",
    "    for _, row in top_outliers.sort_values('growth_rate', ascending=False).iterrows():\n",
    "        title = row['CIP4_title'][:47] + '...' if len(row['CIP4_title']) > 50 else row['CIP4_title']\n",
    "        ai_exp = f\"{row['ai_exposure_score']:.3f}\" if pd.notna(row['ai_exposure_score']) else 'N/A'\n",
    "        print(f\"{row['CIP4']:<8} {row['growth_rate']:>9.2f}% {row['enrollment_2019']:>11,.0f} {row['enrollment_2025']:>11,.0f} {ai_exp:<8} {title:<50}\")\n",
    "\n",
    "    # Print bottom outliers\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BOTTOM {percentile}% GROWTH OUTLIERS (<= {bottom_threshold:.2f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n{'CIP4':<8} {'Growth':<10} {'2019':<12} {'2025':<12} {'AI Exp':<8} {'Title':<50}\")\n",
    "    print(f\"{'-'*8} {'-'*10} {'-'*12} {'-'*12} {'-'*8} {'-'*50}\")\n",
    "    for _, row in bottom_outliers.sort_values('growth_rate', ascending=True).iterrows():\n",
    "        title = row['CIP4_title'][:47] + '...' if len(row['CIP4_title']) > 50 else row['CIP4_title']\n",
    "        ai_exp = f\"{row['ai_exposure_score']:.3f}\" if pd.notna(row['ai_exposure_score']) else 'N/A'\n",
    "        print(f\"{row['CIP4']:<8} {row['growth_rate']:>9.2f}% {row['enrollment_2019']:>11,.0f} {row['enrollment_2025']:>11,.0f} {ai_exp:<8} {title:<50}\")\n",
    "\n",
    "    # Combine outliers\n",
    "    outlier_cip4s = set(top_outliers['CIP4'].tolist() + bottom_outliers['CIP4'].tolist())\n",
    "\n",
    "    # Drop outliers from original dataset\n",
    "    df_clean = df[~df['CIP4'].isin(outlier_cip4s)].copy()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Total majors analyzed: {len(growth_df)}\")\n",
    "    print(f\"  Top outliers dropped: {len(top_outliers)}\")\n",
    "    print(f\"  Bottom outliers dropped: {len(bottom_outliers)}\")\n",
    "    print(f\"  Total outliers dropped: {len(outlier_cip4s)}\")\n",
    "    print(f\"  Remaining majors: {df_clean['CIP4'].nunique()}\")\n",
    "    print(f\"  Observations before: {len(df)}\")\n",
    "    print(f\"  Observations after: {len(df_clean)}\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(\"# AI EXPOSURE AND ENROLLMENT ANALYSIS - 4-DIGIT CIP\")\n",
    "    print(\"#\"*70 + \"\\n\")\n",
    "\n",
    "    # FILE PATHS\n",
    "    FELTEN_PATH = '/Users/jeffreyohl/Dropbox/CollegeMajorData/FeltenEtAl/2023_Language Modeling AIOE and AIIE.xlsx'\n",
    "    CROSSWALK_PATH = '/Users/jeffreyohl/Dropbox/CollegeMajorData/Crosswalks/crosswalk_handout.xlsx'\n",
    "    ACS_PATH = '/Users/jeffreyohl/Dropbox/CollegeMajorData/IPUMS/usa_00011.csv'  # 2013-17 5-year ACS (SOC 2010 codes)\n",
    "    ENROLLMENT_PATH_2025 = '/Users/jeffreyohl/Dropbox/CollegeMajorData/National Student Clearinghouse Data/CTEESpring2025-DataAppendix.xlsx'\n",
    "    ENROLLMENT_PATH_2024 = '/Users/jeffreyohl/Dropbox/CollegeMajorData/National Student Clearinghouse Data/CTEESpring2024-Appendix.xlsx'\n",
    "    OUTPUT_DIR = '/Users/jeffreyohl/Dropbox/CollegeMajorData/output'\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load Felten data\n",
    "        felten = load_felten_data(FELTEN_PATH)\n",
    "\n",
    "        # Step 2: Load FOD to 4-digit CIP crosswalk (with manual mappings)\n",
    "        fod_to_cip4 = load_fod_cip4_crosswalk(CROSSWALK_PATH, manual_mappings=MANUAL_MAPPINGS)\n",
    "\n",
    "        # Step 3: Load and combine enrollment data (2019-2025) - MOVED UP!\n",
    "        enrollment = load_and_combine_enrollment_data(ENROLLMENT_PATH_2024, ENROLLMENT_PATH_2025)\n",
    "\n",
    "        # Step 4: Add empirical enrollment weights to crosswalk\n",
    "        fod_to_cip4_weighted = add_empirical_weights_to_crosswalk(\n",
    "            fod_to_cip4, enrollment, base_year=2019\n",
    "        )\n",
    "\n",
    "        # Step 5: Load and filter ACS data\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PROCESSING ACS DATA\")\n",
    "        print(\"=\"*70)\n",
    "        acs = load_and_filter_acs(ACS_PATH)\n",
    "\n",
    "        # Step 6: Process ACS with exposure\n",
    "        acs_with_exposure = process_acs_with_exposure(acs, felten, fod_to_cip4_weighted)\n",
    "\n",
    "        # DIAGNOSTIC: Show fuzzy matching for top majors\n",
    "        generate_fuzzy_match_diagnostic(acs_with_exposure, top_n_cip4s=None)\n",
    "\n",
    "        cip_exposure = calculate_cip4_exposure(acs_with_exposure)\n",
    "\n",
    "        # Save exposure scores\n",
    "        cip_exposure.to_csv(f'{OUTPUT_DIR}/cip4_ai_exposure_scores.csv', index=False)\n",
    "        print(f\"\\n\u2713 Saved exposure scores to {OUTPUT_DIR}/cip4_ai_exposure_scores.csv\")\n",
    "\n",
    "        # Step 7: Merge enrollment with exposure (NO WAGES YET)\n",
    "        # Normalize CIP4 codes\n",
    "        enrollment['CIP4'] = enrollment['CIP4'].astype(str).str.zfill(4)\n",
    "        cip_exposure['CIP4'] = cip_exposure['CIP4'].astype(str).str.zfill(4)\n",
    "\n",
    "        df_no_wages = enrollment.merge(\n",
    "            cip_exposure[['CIP4', 'ai_exposure_score', 'n_obs']],\n",
    "            on='CIP4',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Create treatment variables (for visualizations)\n",
    "        df_no_wages['high_ai_exposure'] = (df_no_wages['ai_exposure_score'] > df_no_wages['ai_exposure_score'].median()).astype(int)\n",
    "\n",
    "        try:\n",
    "            df_no_wages['ai_exposure_tercile'] = pd.qcut(\n",
    "                df_no_wages['ai_exposure_score'],\n",
    "                q=3,\n",
    "                labels=['Low', 'Medium', 'High'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "        except ValueError:\n",
    "            df_no_wages['ai_exposure_tercile'] = pd.cut(\n",
    "                df_no_wages['ai_exposure_score'],\n",
    "                bins=3,\n",
    "                labels=['Low', 'Medium', 'High']\n",
    "            )\n",
    "\n",
    "        df_no_wages['log_enrollment'] = np.log(df_no_wages['enrollment'] + 1)\n",
    "\n",
    "        # Step 8: Create visualizations (BEFORE wage calculations)\n",
    "        create_descriptive_plots(df_no_wages, f'{OUTPUT_DIR}/enrollment_trends_4digit.png')\n",
    "\n",
    "        # Step 8B: Create tercile deep-dive plots\n",
    "        create_tercile_deepdive_plots(df_no_wages, f'{OUTPUT_DIR}/enrollment_tercile_deepdive.png')\n",
    "\n",
    "        # Step 9: NOW calculate wages (for DiD controls)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CALCULATING WAGES (for DiD controls)\")\n",
    "        print(\"=\"*70)\n",
    "        cip_wages = calculate_cip4_wages(acs, fod_to_cip4_weighted)\n",
    "\n",
    "        # Save wage data\n",
    "        cip_wages.to_csv(f'{OUTPUT_DIR}/cip4_wages_2019.csv', index=False)\n",
    "        print(f\"\\n\u2713 Saved wage data to {OUTPUT_DIR}/cip4_wages_2019.csv\")\n",
    "\n",
    "        # Step 10: Merge wages into dataset\n",
    "        df_final = merge_enrollment_exposure_wages(enrollment, cip_exposure, cip_wages)\n",
    "\n",
    "        # Save final dataset\n",
    "        df_final.to_csv(f'{OUTPUT_DIR}/enrollment_with_ai_exposure_4digit.csv', index=False)\n",
    "        print(f\"\\n\u2713 Saved final dataset to {OUTPUT_DIR}/enrollment_with_ai_exposure_4digit.csv\")\n",
    "\n",
    "        # Step 9: Diagnostic reporting - what's missing?\n",
    "        generate_diagnostic_report(acs, fod_to_cip4_weighted, enrollment)\n",
    "\n",
    "        # Step 10: Drop extreme growth outliers\n",
    "        df_clean = drop_growth_outliers(df_final, percentile=1.0)\n",
    "\n",
    "        # Save cleaned dataset\n",
    "        df_clean.to_csv(f'{OUTPUT_DIR}/enrollment_with_ai_exposure_4digit_clean.csv', index=False)\n",
    "        print(f\"\\n\u2713 Saved cleaned dataset to {OUTPUT_DIR}/enrollment_with_ai_exposure_4digit_clean.csv\")\n",
    "\n",
    "        # Step 11: Run DiD analysis (with base_year parameter)\n",
    "        did_results = run_did_analysis(df_clean, OUTPUT_DIR, base_year=2019)\n",
    "\n",
    "\n",
    "        print(\"\\n\" + \"#\"*70)\n",
    "        print(\"# DATA PREPARATION COMPLETE (4-DIGIT CIP)\")\n",
    "        print(\"#\"*70)\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"1. Review cip4_ai_exposure_scores.csv to validate exposure scores\")\n",
    "        print(\"2. Review cip4_wages_2019.csv to validate wage data\")\n",
    "        print(\"3. Check enrollment_with_ai_exposure_4digit.csv for data quality\")\n",
    "        print(\"4. Review dropped outliers and cleaned dataset\")\n",
    "        print(\"5. Run DiD analysis on cleaned dataset with wage controls\")\n",
    "        print(\"\\n4-digit CIP analysis provides:\")\n",
    "        print(\"  - Computer Science (1107) vs Information Systems (1104)\")\n",
    "        print(\"  - Business Administration (5202) vs Finance (5208) vs Accounting (5203)\")\n",
    "        print(\"  - Wage-controlled DiD for conditional treatment effects\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n\u274c Error: File not found - {e}\")\n",
    "        print(\"\\nPlease check that all data files exist at the specified paths:\")\n",
    "        print(f\"  - Felten: {FELTEN_PATH}\")\n",
    "        print(f\"  - Crosswalk: {CROSSWALK_PATH}\")\n",
    "        print(f\"  - ACS: {ACS_PATH}\")\n",
    "        print(f\"  - Enrollment 2024: {ENROLLMENT_PATH_2024}\")\n",
    "        print(f\"  - Enrollment 2025: {ENROLLMENT_PATH_2025}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}