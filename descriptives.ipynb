{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 manual FOD→CIP4 mappings\n",
      "\n",
      "######################################################################\n",
      "# AI EXPOSURE AND ENROLLMENT ANALYSIS - 4-DIGIT CIP\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "LOADING FELTEN AIOE DATA\n",
      "======================================================================\n",
      "\n",
      "❌ Error occurred: Worksheet named 'Appendix A' not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/ggrmf_5j6tx4rv5qdhs0slq40000gn/T/ipykernel_35349/2066378724.py\", line 939, in main\n",
      "    felten = load_felten_data(FELTEN_PATH)\n",
      "  File \"/var/folders/g9/ggrmf_5j6tx4rv5qdhs0slq40000gn/T/ipykernel_35349/2066378724.py\", line 78, in load_felten_data\n",
      "    felten = pd.read_excel(filepath, sheet_name='Appendix A')\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/excel/_base.py\", line 508, in read_excel\n",
      "    data = io.parse(\n",
      "        sheet_name=sheet_name,\n",
      "    ...<21 lines>...\n",
      "        dtype_backend=dtype_backend,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/excel/_base.py\", line 1616, in parse\n",
      "    return self._reader.parse(\n",
      "           ~~~~~~~~~~~~~~~~~~^\n",
      "        sheet_name=sheet_name,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<17 lines>...\n",
      "        **kwds,\n",
      "        ^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/excel/_base.py\", line 773, in parse\n",
      "    sheet = self.get_sheet_by_name(asheetname)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/excel/_openpyxl.py\", line 582, in get_sheet_by_name\n",
      "    self.raise_if_bad_sheet_by_name(name)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/excel/_base.py\", line 624, in raise_if_bad_sheet_by_name\n",
      "    raise ValueError(f\"Worksheet named '{name}' not found\")\n",
      "ValueError: Worksheet named 'Appendix A' not found\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AI Exposure and College Enrollment Analysis - 4-DIGIT CIP VERSION\n",
    "==================================================================\n",
    "Updated for:\n",
    "- 4-digit CIP codes (436 programs vs 49 at 2-digit level)\n",
    "- 2019-2025 enrollment data (combined from both files)\n",
    "- More granular analysis (e.g., Computer Science vs Information Systems)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION: MANUAL FOD → CIP4 MAPPINGS\n",
    "# =============================================================================\n",
    "# These are mappings not present in the crosswalk file or that need correction.\n",
    "# Each entry: dict with FOD, CIP4, CIP4_title, notes\n",
    "# To add more: just append to this list!\n",
    "\n",
    "MANUAL_MAPPINGS = [\n",
    "    {\n",
    "        'FOD': 6107,\n",
    "        'CIP4': '5138',\n",
    "        'CIP4_title': 'Registered Nursing/Nursing Administration/Nursing Research and Clinical Nursing',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 6107 missing mapping to CIP 5138 (490K students)'\n",
    "    },\n",
    "    {\n",
    "        'FOD': 3611,\n",
    "        'CIP4': '2615',\n",
    "        'CIP4_title': 'Neurobiology and Neurosciences',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 3611 not in original crosswalk'\n",
    "    },\n",
    "    {\n",
    "        'FOD': 5202,\n",
    "        'CIP4': '4228',\n",
    "        'CIP4_title': 'Clinical, Counseling and Applied Psychology',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5202 (Clinical Psychology) maps to CIP 4228'\n",
    "    },\n",
    "    {\n",
    "        'FOD': 5203,\n",
    "        'CIP4': '4228',\n",
    "        'CIP4_title': 'Clinical, Counseling and Applied Psychology',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5203 (Counseling Psychology) maps to CIP 4228'\n",
    "    },\n",
    "\n",
    "      {\n",
    "        'FOD': 5203,\n",
    "        'CIP4': '4228',\n",
    "        'CIP4_title': 'Clinical, Counseling and Applied Psychology',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5203 (Counseling Psychology) maps to CIP 4228'\n",
    "    },\n",
    "     {\n",
    "        'FOD': 5098,\n",
    "        'CIP4': '4099',\n",
    "        'CIP4_title': 'Physical Sciences, other',\n",
    "        'notes': 'Added Nov 6 2025 - FOD 5098 (Multi-disciplinary or General Science) maps to CIP 4099'\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(MANUAL_MAPPINGS)} manual FOD→CIP4 mappings\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD FELTEN AIOE DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_felten_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load Felten et al. (2021) AIOE scores.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING FELTEN AIOE DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    felten = pd.read_excel(filepath, sheet_name='Appendix A')\n",
    "    felten['soc_clean'] = felten['SOC Code'].str.replace('-', '').str.replace('.', '')\n",
    "    \n",
    "    print(f\"\\nLoaded {len(felten)} occupations\")\n",
    "    print(f\"AIOE range: {felten['AIOE'].min():.2f} to {felten['AIOE'].max():.2f}\")\n",
    "    \n",
    "    return felten\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: LOAD FOD TO 4-DIGIT CIP CROSSWALK\n",
    "# =============================================================================\n",
    "\n",
    "def load_fod_cip4_crosswalk(filepath: str, manual_mappings: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load FOD to 4-digit CIP mapping from crosswalk file.\n",
    "    \n",
    "    The crosswalk has detailed 6-digit CIP codes (like 11.0701).\n",
    "    We extract 4-digit CIP:\n",
    "    - Family (2 digits): 11 = Computer Science\n",
    "    - Group (next 2 digits): 07 = Computer Science \n",
    "    - Combined: 1107 = Computer Science (4-digit)\n",
    "    \n",
    "    Examples:\n",
    "    - 11.0000 → 1100 (Computer Science, General)\n",
    "    - 11.0701 → 1107 (Computer Science)\n",
    "    - 52.0201 → 5202 (Business Administration)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to crosswalk Excel file\n",
    "    manual_mappings : list of dict\n",
    "        Additional manual mappings to append. Each dict should have keys:\n",
    "        'FOD', 'CIP4', 'CIP4_title', 'notes'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with columns ['FOD', 'CIP4', 'CIP4_title']\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING FOD TO 4-DIGIT CIP CROSSWALK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Read from \"CIP code by HHES code\" sheet\n",
    "    df = pd.read_excel(filepath, sheet_name='CIP code by HHES code', skiprows=1)\n",
    "    \n",
    "    # Extract FOD and CIP columns\n",
    "    crosswalk = df[['HHES Code', 'CIP \\nCode', 'CIP Title']].copy()\n",
    "    crosswalk.columns = ['FOD', 'CIP', 'CIP_title']\n",
    "    crosswalk = crosswalk.dropna(subset=['FOD', 'CIP'])\n",
    "    \n",
    "    # Convert FOD to integer\n",
    "    crosswalk['FOD'] = crosswalk['FOD'].astype(int)\n",
    "    \n",
    "    # Extract 4-digit CIP from 6-digit CIP code\n",
    "    # CIP format: XX.XXXX where first 2 are family, next 2 are group\n",
    "    # E.g., 11.0701 → 1107\n",
    "    def extract_cip4(cip_6digit):\n",
    "        try:\n",
    "            cip_float = float(cip_6digit)\n",
    "            # Get integer part (family, 2 digits)\n",
    "            family = int(cip_float)  # e.g., 11\n",
    "            # Get first 2 decimal digits (group)\n",
    "            decimal_part = cip_float - family  # e.g., 0.0701\n",
    "            # Extract first 2 decimal digits\n",
    "            group = int(round(decimal_part * 10000)) // 100  # e.g., 07\n",
    "            # Combine to 4-digit code\n",
    "            cip4 = f\"{family:02d}{group:02d}\"  # e.g., \"1107\"\n",
    "            return cip4\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    crosswalk['CIP4'] = crosswalk['CIP'].apply(extract_cip4)\n",
    "    crosswalk = crosswalk.dropna(subset=['CIP4'])\n",
    "    \n",
    "    # Create many-to-many mapping (each FOD can map to multiple CIP4s)\n",
    "    # Keep CIP4_title for the most general title per CIP4\n",
    "    fod_to_cip4_df = crosswalk.groupby(['FOD', 'CIP4']).agg({\n",
    "        'CIP_title': 'first'  # Take first title (they're usually the same for same CIP4)\n",
    "    }).reset_index()\n",
    "    fod_to_cip4_df.columns = ['FOD', 'CIP4', 'CIP4_title']\n",
    "    \n",
    "    print(f\"\\nLoaded {len(fod_to_cip4_df)} FOD→CIP4 mappings from crosswalk file\")\n",
    "    print(f\"  {fod_to_cip4_df['FOD'].nunique()} unique FODs\")\n",
    "    print(f\"  {fod_to_cip4_df['CIP4'].nunique()} unique CIP4 codes\")\n",
    "    print(f\"  Average {len(fod_to_cip4_df) / fod_to_cip4_df['FOD'].nunique():.1f} CIP4 codes per FOD\")\n",
    "    \n",
    "    # Append manual mappings if provided\n",
    "    if manual_mappings:\n",
    "        manual_df = pd.DataFrame(manual_mappings)[['FOD', 'CIP4', 'CIP4_title']]\n",
    "        fod_to_cip4_df = pd.concat([fod_to_cip4_df, manual_df], ignore_index=True)\n",
    "        print(f\"\\n✓ Added {len(manual_mappings)} manual mappings\")\n",
    "        for mapping in manual_mappings:\n",
    "            print(f\"  FOD {mapping['FOD']} → CIP4 {mapping['CIP4']} ({mapping['CIP4_title']})\")\n",
    "    \n",
    "    # Show sample mappings\n",
    "    print(\"\\nSample mappings:\")\n",
    "    sample_fods = sorted(fod_to_cip4_df['FOD'].unique())[:10]\n",
    "    for fod in sample_fods:\n",
    "        cips = fod_to_cip4_df[fod_to_cip4_df['FOD'] == fod]['CIP4'].tolist()\n",
    "        print(f\"  FOD {fod} → CIP4 {cips}\")\n",
    "    \n",
    "    return fod_to_cip4_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2B: ADD EMPIRICAL ENROLLMENT WEIGHTS TO FOD→CIP4 MAPPING\n",
    "# =============================================================================\n",
    "\n",
    "def add_empirical_weights_to_crosswalk(\n",
    "    fod_to_cip4: pd.DataFrame,\n",
    "    enrollment: pd.DataFrame,\n",
    "    base_year: int = 2019\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add empirical enrollment weights to FOD→CIP4 mapping.\n",
    "    \n",
    "    For each FOD that maps to multiple CIP4s, calculate weights based on\n",
    "    actual 2019 enrollment: weight_i = enrollment_i / sum(enrollment for all CIP4s that FOD maps to)\n",
    "    \n",
    "    This creates a Bayesian update: P(CIP4 | FOD) ∝ enrollment(CIP4)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fod_to_cip4 : DataFrame with columns ['FOD', 'CIP4', 'CIP4_title']\n",
    "    enrollment : DataFrame with columns ['CIP4', 'year', 'enrollment']\n",
    "    base_year : Year to use for calculating weights (default 2019)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with columns ['FOD', 'CIP4', 'CIP4_title', 'empirical_weight']\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ADDING EMPIRICAL ENROLLMENT WEIGHTS TO FOD→CIP4 MAPPING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get base year enrollment\n",
    "    enroll_base = enrollment[enrollment['year'] == base_year][['CIP4', 'enrollment', 'CIP4_title']].copy()\n",
    "    print(f\"\\nUsing {base_year} enrollment as basis for weights\")\n",
    "    print(f\"  {len(enroll_base)} CIP4 codes have enrollment data\")\n",
    "    \n",
    "    # Merge enrollment into crosswalk\n",
    "    crosswalk_with_enroll = fod_to_cip4.merge(\n",
    "        enroll_base[['CIP4', 'enrollment']],\n",
    "        on='CIP4',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # For CIP4s with no enrollment data, use a small value (1.0) as placeholder\n",
    "    crosswalk_with_enroll['enrollment'] = crosswalk_with_enroll['enrollment'].fillna(1.0)\n",
    "    \n",
    "    # For each FOD, calculate weights as proportion of total enrollment\n",
    "    # weight_i = enrollment_i / sum_j(enrollment_j) for all j that FOD maps to\n",
    "    fod_totals = crosswalk_with_enroll.groupby('FOD')['enrollment'].transform('sum')\n",
    "    crosswalk_with_enroll['empirical_weight'] = crosswalk_with_enroll['enrollment'] / fod_totals\n",
    "    \n",
    "    # Clean up\n",
    "    crosswalk_weighted = crosswalk_with_enroll[['FOD', 'CIP4', 'CIP4_title', 'empirical_weight']].copy()\n",
    "    \n",
    "    # Report\n",
    "    print(f\"\\nCalculated empirical weights for {len(crosswalk_weighted)} FOD→CIP4 mappings\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nSample weighted mappings:\")\n",
    "    sample_fods = crosswalk_weighted['FOD'].unique()[:3]\n",
    "    for fod in sample_fods:\n",
    "        fod_mappings = crosswalk_weighted[crosswalk_weighted['FOD'] == fod]\n",
    "        print(f\"\\n  FOD {fod} maps to {len(fod_mappings)} CIP4 codes:\")\n",
    "        for _, row in fod_mappings.iterrows():\n",
    "            print(f\"    CIP4 {row['CIP4']}: weight = {row['empirical_weight']:.3f}\")\n",
    "    \n",
    "    # Sanity check: weights should sum to 1.0 for each FOD\n",
    "    weight_sums = crosswalk_weighted.groupby('FOD')['empirical_weight'].sum()\n",
    "    if not np.allclose(weight_sums, 1.0):\n",
    "        print(f\"\\n⚠ WARNING: Some FOD weights don't sum to 1.0!\")\n",
    "        print(f\"  Min: {weight_sums.min():.6f}, Max: {weight_sums.max():.6f}\")\n",
    "    else:\n",
    "        print(f\"\\n✓ All FOD weights sum to 1.0\")\n",
    "    \n",
    "    return crosswalk_weighted\n",
    "\n",
    "\n",
    "# STEP 3: LOAD AND PROCESS ACS PUMS DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_filter_acs(\n",
    "    filepath: str,\n",
    "    age_min: int = 22,\n",
    "    age_max: int = 35\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and filter IPUMS ACS PUMS data.\n",
    "    \n",
    "    Your ACS columns: DEGFIELDD, OCCSOC, PERWT, AGE, EDUC, YEAR\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING ACS PUMS DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    acs = pd.read_csv(filepath)\n",
    "    \n",
    "    print(f\"\\nInitial sample: {len(acs):,} observations\")\n",
    "    \n",
    "    # Filter out missing, invalid, and zero FODs\n",
    "    acs_filtered = acs[\n",
    "        (acs['AGE'] >= age_min) & \n",
    "        (acs['AGE'] <= age_max) &\n",
    "        (acs['OCCSOC'].notna()) &\n",
    "        (acs['DEGFIELDD'].notna()) &\n",
    "        (acs['DEGFIELDD'] != 0)  # Exclude FOD = 0 (invalid/no field of degree)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Filtered sample: {len(acs_filtered):,} observations\")\n",
    "    print(f\"  - Age {age_min}-{age_max}\")\n",
    "    print(f\"  - Valid occupation (OCCSOC) and field of degree (DEGFIELDD)\")\n",
    "    \n",
    "    # Clean SOC codes\n",
    "    acs_filtered['soc_clean'] = acs_filtered['OCCSOC'].astype(str).str.replace('-', '').str.replace('.', '')\n",
    "    \n",
    "    print(f\"\\nUnique DEGFIELDD codes: {acs_filtered['DEGFIELDD'].nunique()}\")\n",
    "    print(f\"Unique OCCSOC codes: {acs_filtered['OCCSOC'].nunique()}\")\n",
    "    \n",
    "    return acs_filtered\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: MAP FOD TO 4-DIGIT CIP AND MERGE WITH EXPOSURE\n",
    "# =============================================================================\n",
    "\n",
    "def process_acs_with_exposure(\n",
    "    acs: pd.DataFrame,\n",
    "    felten: pd.DataFrame,\n",
    "    fod_to_cip4: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map ACS FOD codes to 4-digit CIP using many-to-many relationship.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MAPPING FOD TO 4-DIGIT CIP AND MERGING AI EXPOSURE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Map FOD to CIP4\n",
    "    # Map FOD to CIP4 using many-to-many relationship\n",
    "    # Each ACS observation can contribute to multiple CIP4 codes\n",
    "    acs_with_cip = acs.merge(\n",
    "        fod_to_cip4,\n",
    "        left_on='DEGFIELDD',\n",
    "        right_on='FOD',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Report mapping success\n",
    "    n_original = len(acs)\n",
    "    n_after_mapping = len(acs_with_cip)\n",
    "    n_unique_people = acs_with_cip['DEGFIELDD'].nunique() if 'DEGFIELDD' in acs_with_cip.columns else len(acs_with_cip)\n",
    "    \n",
    "    # Use empirical weights from crosswalk (already calculated based on 2019 enrollment)\n",
    "    # Each ACS person contributes weight_split = PERWT * empirical_weight to each CIP4\n",
    "    acs_with_cip['weight_split'] = acs_with_cip['PERWT'] * acs_with_cip['empirical_weight']\n",
    "    \n",
    "    avg_cips = len(acs_with_cip) / len(acs)\n",
    "    print(f\"  Each ACS person contributes to avg {avg_cips:.1f} CIP4 codes (weighted by 2019 enrollment)\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\nMapped {n_unique_people:,} ACS observations to {len(acs_with_cip):,} CIP4 mappings\")\n",
    "    print(f\"  (Average {n_after_mapping/n_unique_people if n_unique_people > 0 else 0:.1f} CIP4 codes per person)\")\n",
    "    \n",
    "    # Check unmapped FODs\n",
    "    if len(acs_with_cip) < len(acs):\n",
    "        unmapped = acs[~acs['DEGFIELDD'].isin(fod_to_cip4['FOD'])]\n",
    "        unmapped_fods = unmapped['DEGFIELDD'].value_counts().head(10)\n",
    "        print(\"\\nTop 10 unmapped FOD codes:\")\n",
    "        print(unmapped_fods)\n",
    "    \n",
    "    \n",
    "    # Filter to successfully mapped\n",
    "    \n",
    "    # Merge with Felten AIOE scores on SOC code\n",
    "    acs_with_exposure = acs_with_cip.merge(\n",
    "        felten[['soc_clean', 'AIOE']],\n",
    "        on='soc_clean',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Report merge success\n",
    "    n_matched = acs_with_exposure['AIOE'].notna().sum()\n",
    "    pct_matched = 100 * n_matched / len(acs_with_exposure)\n",
    "    print(f\"\\nMatched {n_matched:,}/{len(acs_with_exposure):,} observations to AI exposure ({pct_matched:.1f}%)\")\n",
    "    \n",
    "    \n",
    "    # Drop observations with missing AIOE (do NOT impute with mean)\n",
    "    n_missing = acs_with_exposure['AIOE'].isna().sum()\n",
    "    if n_missing > 0:\n",
    "        print(f\"\\n⚠ Dropping {n_missing:,} observations with missing AIOE scores\")\n",
    "        acs_with_exposure = acs_with_exposure[acs_with_exposure['AIOE'].notna()].copy()\n",
    "    \n",
    "    print(f\"\\nFinal sample: {len(acs_with_exposure):,} observations\")\n",
    "    print(f\"Unique 4-digit CIP codes: {acs_with_exposure['CIP4'].nunique()}\")\n",
    "    \n",
    "    return acs_with_exposure\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: CALCULATE 4-DIGIT CIP-LEVEL AI EXPOSURE\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_cip4_exposure(\n",
    "    acs: pd.DataFrame,\n",
    "    weight_var: str = 'weight_split'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate weighted average AI exposure by 4-digit CIP code.\n",
    "    \n",
    "    For each CIP4: AI_exposure = Σ [P(occupation|CIP4) × AIOE(occupation)]\n",
    "    where P(occupation|CIP4) is weighted by split weights (PERWT × empirical_weight).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CALCULATING 4-DIGIT CIP-LEVEL AI EXPOSURE SCORES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate weighted average by CIP4\n",
    "    cip_exposure = acs.groupby('CIP4').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'ai_exposure_score': np.average(x['AIOE'], weights=x[weight_var]),\n",
    "            'n_obs': len(x),\n",
    "            'n_weighted': x[weight_var].sum(),\n",
    "            'min_exposure': x['AIOE'].min(),\n",
    "            'max_exposure': x['AIOE'].max(),\n",
    "            'std_exposure': np.sqrt(np.average((x['AIOE'] - np.average(x['AIOE'], weights=x[weight_var]))**2, \n",
    "                                                weights=x[weight_var])),\n",
    "            'CIP4_title': x['CIP4_title'].iloc[0] if 'CIP4_title' in x.columns else ''\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(f\"\\nCalculated AI exposure for {len(cip_exposure)} 4-digit CIP codes\")\n",
    "    print(\"\\nAI Exposure Score Distribution:\")\n",
    "    print(cip_exposure['ai_exposure_score'].describe())\n",
    "    \n",
    "    # Show top and bottom CIPs\n",
    "    print(\"\\n\\nTop 20 most AI-exposed majors (4-digit CIP):\")\n",
    "    top20 = cip_exposure.nlargest(20, 'ai_exposure_score')[['CIP4', 'CIP4_title', 'ai_exposure_score', 'n_obs']]\n",
    "    print(top20.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\\nBottom 20 least AI-exposed majors (4-digit CIP):\")\n",
    "    bottom20 = cip_exposure.nsmallest(20, 'ai_exposure_score')[['CIP4', 'CIP4_title', 'ai_exposure_score', 'n_obs']]\n",
    "    print(bottom20.to_string(index=False))\n",
    "    \n",
    "    return cip_exposure\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: LOAD AND COMBINE ENROLLMENT DATA (2019-2025)\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_combine_enrollment_data(\n",
    "    filepath_2024: str, \n",
    "    filepath_2025: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine enrollment data from two sources with proper header handling.\n",
    "    \n",
    "    2024 file: Major Field (4-year, Undergrad) sheet, years 2019-2024\n",
    "    2025 file: CIP Group Enrollment sheet, years 2020-2025 (filter to Undergraduate 4-year)\n",
    "    \n",
    "    Returns combined dataset with 4-digit CIP codes (2019-2025), including CIP4_title.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING AND COMBINING ENROLLMENT DATA (2019-2025)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ===== LOAD 2024 FILE =====\n",
    "    print(\"\\nLoading 2019-2024 data from CTEESpring2024-Appendix.xlsx...\")\n",
    "    df_2024 = pd.read_excel(\n",
    "        filepath_2024, \n",
    "        sheet_name='Major Field (4-year, Undergrad)',\n",
    "        header=2  # Row 2 has the actual column headers\n",
    "    )\n",
    "    print(f\"  Loaded {len(df_2024)} rows\")\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df_2024 = df_2024.rename(columns={\n",
    "        'Major Field Family (2-digit CIP)': 'CIP2',\n",
    "        'Major Field Family (2-digit) Title': 'CIP2_title',\n",
    "        'Major Field Group (4-digit CIP)': 'CIP4',\n",
    "        'Major Field Group (4-digit) Title': 'CIP4_title'\n",
    "    })\n",
    "    \n",
    "    # Get enrollment columns (years 2019-2024)\n",
    "    years_2024 = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "    enrollment_cols = [col for col in df_2024.columns if 'Enrollment' in str(col) and '% Change' not in str(col)]\n",
    "    print(f\"  Found {len(enrollment_cols)} enrollment columns for years 2019-2024\")\n",
    "    \n",
    "    # Reshape to long format\n",
    "    data_2024 = []\n",
    "    for idx, row in df_2024.iterrows():\n",
    "        cip4 = row['CIP4']\n",
    "        cip4_title = row['CIP4_title']\n",
    "        if pd.isna(cip4) or cip4 == 'Total':\n",
    "            continue\n",
    "        for year, col in zip(years_2024, enrollment_cols):\n",
    "            enrollment = row[col]\n",
    "            if pd.notna(enrollment) and enrollment != '*':\n",
    "                data_2024.append({\n",
    "                    'CIP4': str(cip4)[:4] if pd.notna(cip4) and str(cip4) != 'Total' else None,\n",
    "                    'CIP4_title': cip4_title,\n",
    "                    'year': year,\n",
    "                    'enrollment': float(enrollment)\n",
    "                })\n",
    "    \n",
    "    df_2024_long = pd.DataFrame(data_2024)\n",
    "    print(f\"  Reshaped to {len(df_2024_long)} observations\")\n",
    "    \n",
    "    # ===== LOAD 2025 FILE =====\n",
    "    print(\"\\nLoading 2020-2025 data from CTEESpring2025-DataAppendix.xlsx...\")\n",
    "    df_2025 = pd.read_excel(\n",
    "        filepath_2025,\n",
    "        sheet_name='CIP Group Enrollment',\n",
    "        header=2  # Row 2 has the actual column headers\n",
    "    )\n",
    "    print(f\"  Loaded {len(df_2025)} rows\")\n",
    "    \n",
    "    # Filter to Undergraduate 4-year only\n",
    "    df_2025 = df_2025[df_2025['Award Level and Institution Type'] == 'Undergraduate 4-year'].copy()\n",
    "    print(f\"  Filtered to {len(df_2025)} Undergraduate 4-year rows\")\n",
    "    \n",
    "    # Rename columns\n",
    "    df_2025 = df_2025.rename(columns={\n",
    "        'Major Field Family \\n(2-digit CIP)': 'CIP2',\n",
    "        'Major Field Family \\n(2-digit CIP) Title': 'CIP2_title',\n",
    "        'Major Field Group \\n(4-digit CIP)': 'CIP4',\n",
    "        'Major Field Group \\n(4-digit CIP) Title': 'CIP4_title'\n",
    "    })\n",
    "    \n",
    "    # Get enrollment columns (years 2020-2025)\n",
    "    # The enrollment columns alternate: Enrollment, % Change, Enrollment, % Change...\n",
    "    # Columns 5, 6, 8, 10, 12, 14 correspond to years 2020-2025\n",
    "    years_2025 = [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "    enrollment_col_indices = [5, 6, 8, 10, 12, 14]\n",
    "    \n",
    "    # Reshape to long format\n",
    "    data_2025 = []\n",
    "    for idx, row in df_2025.iterrows():\n",
    "        cip4 = row['CIP4']\n",
    "        cip4_title = row['CIP4_title']\n",
    "        if pd.isna(cip4) or cip4 == 'Total':\n",
    "            continue\n",
    "        for year, col_idx in zip(years_2025, enrollment_col_indices):\n",
    "            enrollment = row.iloc[col_idx]\n",
    "            if pd.notna(enrollment) and enrollment != '*':\n",
    "                data_2025.append({\n",
    "                    'CIP4': str(cip4)[:4] if pd.notna(cip4) and str(cip4) != 'Total' else None,\n",
    "                    'CIP4_title': cip4_title,\n",
    "                    'year': year,\n",
    "                    'enrollment': float(enrollment)\n",
    "                })\n",
    "    \n",
    "    df_2025_long = pd.DataFrame(data_2025)\n",
    "    print(f\"  Reshaped to {len(df_2025_long)} observations\")\n",
    "    \n",
    "    # ===== COMBINE DATASETS =====\n",
    "    print(\"\\nCombining datasets...\")\n",
    "    \n",
    "    # For overlapping years (2020-2024), use 2025 file data (more recent)\n",
    "    df_2024_unique = df_2024_long[df_2024_long['year'] == 2019].copy()\n",
    "    \n",
    "    enrollment = pd.concat([df_2024_unique, df_2025_long], axis=0, ignore_index=True)\n",
    "    enrollment = enrollment.sort_values(['CIP4', 'year']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n✓ Combined dataset: {len(enrollment)} observations\")\n",
    "    print(f\"  Years: {sorted(enrollment['year'].unique())}\")\n",
    "    print(f\"  Unique 4-digit CIP codes: {enrollment['CIP4'].nunique()}\")\n",
    "    \n",
    "    # Summary stats\n",
    "    print(\"\\nTotal enrollment by year:\")\n",
    "    yearly_enrollment = enrollment.groupby('year')['enrollment'].sum()\n",
    "    for year, total in yearly_enrollment.items():\n",
    "        print(f\"  {year}: {total:,.0f}\")\n",
    "    \n",
    "    return enrollment\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: MERGE AND FINALIZE\n",
    "# =============================================================================\n",
    "\n",
    "def merge_enrollment_exposure(\n",
    "    enrollment: pd.DataFrame,\n",
    "    cip_exposure: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge enrollment data with AI exposure scores.\n",
    "    Preserves CIP4_title from enrollment data (more complete).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MERGING ENROLLMENT WITH AI EXPOSURE\")\n",
    "    print(\"=\"*70)\n",
    "    # Normalize CIP4 codes to match (both as zero-padded 4-char strings)\n",
    "    enrollment['CIP4'] = enrollment['CIP4'].astype(str).str.zfill(4)\n",
    "    cip_exposure['CIP4'] = cip_exposure['CIP4'].astype(str).str.zfill(4)\n",
    "    \n",
    "    \n",
    "    # Merge on 4-digit CIP code\n",
    "    # Keep CIP4_title from enrollment (left) as it's more complete\n",
    "    df_final = enrollment.merge(\n",
    "        cip_exposure[['CIP4', 'ai_exposure_score', 'n_obs']],\n",
    "        on='CIP4',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Report merge success\n",
    "    n_matched = df_final['ai_exposure_score'].notna().sum()\n",
    "    pct_matched = 100 * n_matched / len(df_final)\n",
    "    print(f\"\\nMatched {n_matched}/{len(df_final)} enrollment records ({pct_matched:.1f}%)\")\n",
    "    \n",
    "    # Create treatment variables\n",
    "    median_exposure = df_final['ai_exposure_score'].median()\n",
    "    df_final['high_ai_exposure'] = (\n",
    "        df_final['ai_exposure_score'] > median_exposure\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Standardized exposure\n",
    "    df_final['ai_exposure_std'] = (\n",
    "        (df_final['ai_exposure_score'] - df_final['ai_exposure_score'].mean()) /\n",
    "        df_final['ai_exposure_score'].std()\n",
    "    )\n",
    "    \n",
    "    # Terciles (with error handling for insufficient unique values)\n",
    "    try:\n",
    "        df_final['ai_exposure_tercile'] = pd.qcut(\n",
    "            df_final['ai_exposure_score'],\n",
    "            q=3,\n",
    "            labels=['Low', 'Medium', 'High'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        # If qcut fails (e.g., too many NaNs or duplicates), use simple cut\n",
    "        print(f\"⚠ Could not create terciles: {e}\")\n",
    "        print(\"  Using quartile-based cut instead\")\n",
    "        df_final['ai_exposure_tercile'] = pd.cut(\n",
    "            df_final['ai_exposure_score'],\n",
    "            bins=3,\n",
    "            labels=['Low', 'Medium', 'High']\n",
    "        )\n",
    "    \n",
    "    # Create log enrollment\n",
    "    df_final['log_enrollment'] = np.log(df_final['enrollment'] + 1)\n",
    "    \n",
    "    print(\"\\n\\nFinal dataset:\")\n",
    "    print(df_final.head(20))\n",
    "    print(f\"\\nShape: {df_final.shape}\")\n",
    "    print(f\"Columns: {list(df_final.columns)}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_descriptive_plots(df: pd.DataFrame, output_path: str = 'enrollment_trends_4digit.png'):\n",
    "    \"\"\"\n",
    "    Create descriptive visualizations for 4-digit CIP analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Enrollment trends by AI exposure group\n",
    "    ax1 = axes[0, 0]\n",
    "    trend_data = df.groupby(['year', 'high_ai_exposure'])['enrollment'].sum().reset_index()\n",
    "    # Normalize to 2019 (show as % of 2019 enrollment)\n",
    "    trend_2019 = trend_data[trend_data['year'] == 2019].set_index('high_ai_exposure')['enrollment']\n",
    "    trend_data['enrollment_pct_2019'] = trend_data.apply(\n",
    "        lambda row: (row['enrollment'] / trend_2019[row['high_ai_exposure']]) * 100\n",
    "            if row['high_ai_exposure'] in trend_2019.index else 100,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    for group in [0, 1]:\n",
    "        data = trend_data[trend_data['high_ai_exposure'] == group]\n",
    "        label = 'High AI Exposure' if group else 'Low AI Exposure'\n",
    "        ax1.plot(data['year'], data['enrollment_pct_2019'], marker='o', label=label, linewidth=2)\n",
    "    ax1.set_xlabel('Year', fontsize=12)\n",
    "    ax1.set_ylabel('Enrollment (% of 2019)', fontsize=12)\n",
    "    ax1.set_title('Enrollment Trends by AI Exposure (4-digit CIP)', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.axvline(2022.5, color='red', linestyle='--', alpha=0.5, label='ChatGPT Launch')\n",
    "    \n",
    "    # 2. Distribution of AI exposure\n",
    "    ax2 = axes[0, 1]\n",
    "    cip_scores = df.groupby('CIP4')['ai_exposure_score'].first()\n",
    "    ax2.hist(cip_scores, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax2.axvline(cip_scores.median(), color='red', linestyle='--', linewidth=2, label='Median')\n",
    "    ax2.set_xlabel('AI Exposure Score', fontsize=12)\n",
    "    ax2.set_ylabel('Number of 4-digit CIP Codes', fontsize=12)\n",
    "    ax2.set_title('Distribution of AI Exposure Across Majors', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Scatter: enrollment growth vs exposure\n",
    "    ax3 = axes[1, 0]\n",
    "    first_year = df['year'].min()\n",
    "    last_year = df['year'].max()\n",
    "    \n",
    "    growth_data = []\n",
    "    for cip in df['CIP4'].unique():\n",
    "        cip_data = df[df['CIP4'] == cip]\n",
    "        enroll_first = cip_data[cip_data['year'] == first_year]['enrollment'].values\n",
    "        enroll_last = cip_data[cip_data['year'] == last_year]['enrollment'].values\n",
    "        if len(enroll_first) > 0 and len(enroll_last) > 0 and enroll_first[0] > 0:\n",
    "            growth = (enroll_last[0] - enroll_first[0]) / enroll_first[0] * 100\n",
    "            exposure = cip_data['ai_exposure_score'].iloc[0] if len(cip_data) > 0 else None\n",
    "            if exposure is not None and pd.notna(exposure):\n",
    "                growth_data.append({'CIP4': cip, 'growth_rate': growth, 'ai_exposure': exposure})\n",
    "    \n",
    "    growth_df = pd.DataFrame(growth_data)\n",
    "    if len(growth_df) > 0:\n",
    "        ax3.scatter(growth_df['ai_exposure'], growth_df['growth_rate'], alpha=0.6, s=30)\n",
    "        ax3.set_xlabel('AI Exposure Score', fontsize=12)\n",
    "        ax3.set_ylabel(f'Enrollment Growth Rate ({first_year}-{last_year}, %)', fontsize=12)\n",
    "        ax3.set_title('Growth Rate vs AI Exposure', fontsize=14, fontweight='bold')\n",
    "        ax3.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax3.grid(alpha=0.3)\n",
    "        \n",
    "        # Add correlation\n",
    "        corr = growth_df[['ai_exposure', 'growth_rate']].corr().iloc[0, 1]\n",
    "        ax3.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                transform=ax3.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 4. Enrollment by tercile over time\n",
    "    ax4 = axes[1, 1]\n",
    "    tercile_data = df.groupby(['year', 'ai_exposure_tercile'])['enrollment'].sum().reset_index()\n",
    "    # Normalize to 2019\n",
    "    tercile_2019 = tercile_data[tercile_data['year'] == 2019].set_index('ai_exposure_tercile')['enrollment']\n",
    "    tercile_data['enrollment_pct_2019'] = tercile_data.apply(\n",
    "        lambda row: (row['enrollment'] / tercile_2019[row['ai_exposure_tercile']]) * 100\n",
    "            if row['ai_exposure_tercile'] in tercile_2019.index else 100,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    for tercile in ['Low', 'Medium', 'High']:\n",
    "        data = tercile_data[tercile_data['ai_exposure_tercile'] == tercile]\n",
    "        if len(data) > 0:\n",
    "            ax4.plot(data['year'], data['enrollment_pct_2019'], marker='o', label=f'{tercile} Exposure', linewidth=2)\n",
    "    ax4.set_xlabel('Year', fontsize=12)\n",
    "    ax4.set_ylabel('Enrollment (% of 2019)', fontsize=12)\n",
    "    ax4.set_title('Enrollment by AI Exposure Tercile', fontsize=14, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(alpha=0.3)\n",
    "    ax4.axvline(2022.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved plots to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8B: TERCILE DEEP-DIVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_tercile_deepdive_plots(df: pd.DataFrame, output_path: str = 'enrollment_tercile_deepdive.png'):\n",
    "    \"\"\"\n",
    "    Create detailed enrollment trend plots for top 10 majors within each AI exposure tercile.\n",
    "    \n",
    "    For each tercile (Low/Medium/High), shows:\n",
    "    - Top 10 CIP4 codes by 2019 enrollment\n",
    "    - Enrollment trends 2019-2025 (normalized to 2019 = 100%)\n",
    "    - CIP4 labels with titles\n",
    "    - % coverage: what fraction of tercile enrollment these top 10 represent\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING TERCILE DEEP-DIVE PLOTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Filter to rows with valid tercile assignment\n",
    "    df_valid = df[df['ai_exposure_tercile'].notna()].copy()\n",
    "    \n",
    "    # Create figure with 3 subplots (1 row x 3 cols)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Color palette for 10 lines\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    for idx, tercile in enumerate(['Low', 'Medium', 'High']):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get data for this tercile\n",
    "        tercile_data = df_valid[df_valid['ai_exposure_tercile'] == tercile].copy()\n",
    "        \n",
    "        if len(tercile_data) == 0:\n",
    "            print(f\"⚠ No data for {tercile} tercile, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get 2019 baseline\n",
    "        tercile_2019 = tercile_data[tercile_data['year'] == 2019].copy()\n",
    "        \n",
    "        # Get top 10 CIP4 by 2019 enrollment\n",
    "        top10_cip4s = tercile_2019.nlargest(10, 'enrollment')['CIP4'].values\n",
    "        \n",
    "        # Calculate coverage\n",
    "        top10_enrollment = tercile_2019[tercile_2019['CIP4'].isin(top10_cip4s)]['enrollment'].sum()\n",
    "        total_enrollment = tercile_2019['enrollment'].sum()\n",
    "        coverage_pct = (top10_enrollment / total_enrollment * 100) if total_enrollment > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{tercile} Tercile:\")\n",
    "        print(f\"  Top 10 CIP4s: {list(top10_cip4s)}\")\n",
    "        print(f\"  Coverage: {coverage_pct:.1f}% of {tercile} tercile enrollment\")\n",
    "        print(f\"  2019 enrollment in top 10: {top10_enrollment:,.0f} / {total_enrollment:,.0f}\")\n",
    "        \n",
    "        # For each of the top 10 CIP4s, plot enrollment trend\n",
    "        for i, cip4 in enumerate(top10_cip4s):\n",
    "            cip_data = tercile_data[tercile_data['CIP4'] == cip4].copy()\n",
    "            \n",
    "            if len(cip_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get 2019 baseline for this CIP4\n",
    "            baseline_2019 = cip_data[cip_data['year'] == 2019]['enrollment'].values\n",
    "            if len(baseline_2019) == 0 or baseline_2019[0] == 0:\n",
    "                continue\n",
    "            baseline_2019 = baseline_2019[0]\n",
    "            \n",
    "            # Normalize to 2019 = 100%\n",
    "            cip_data['enrollment_pct'] = (cip_data['enrollment'] / baseline_2019) * 100\n",
    "            \n",
    "            # Get CIP4 title (truncate if too long)\n",
    "            cip4_title = cip_data['CIP4_title'].iloc[0] if len(cip_data) > 0 else ''\n",
    "            if len(cip4_title) > 30:\n",
    "                cip4_title = cip4_title[:27] + '...'\n",
    "            \n",
    "            # Plot\n",
    "            label = f\"{cip4}: {cip4_title}\"\n",
    "            ax.plot(cip_data['year'], cip_data['enrollment_pct'], \n",
    "                   marker='o', label=label, linewidth=2, color=colors[i], alpha=0.8)\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_xlabel('Year', fontsize=11)\n",
    "        ax.set_ylabel('Enrollment (% of 2019)', fontsize=11)\n",
    "        ax.set_title(f'{tercile} AI Exposure - Top 10 Majors', fontsize=13, fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.axvline(2022.5, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "        ax.axhline(100, color='gray', linestyle=':', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        # Add coverage annotation\n",
    "        annotation_text = f\"Top 10: {coverage_pct:.1f}% of tercile\\nN = {top10_enrollment:,.0f} (2019)\"\n",
    "        ax.text(0.02, 0.98, annotation_text, \n",
    "               transform=ax.transAxes, \n",
    "               verticalalignment='top',\n",
    "               fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "        \n",
    "        # Legend - smaller font, outside plot\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0, -0.15), \n",
    "                 ncol=1, fontsize=8, framealpha=0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved tercile deep-dive plots to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# # STEP 9: DIAGNOSTIC REPORTING\n",
    "# =============================================================================\n",
    "\n",
    "def generate_diagnostic_report(\n",
    "    acs: pd.DataFrame,\n",
    "    fod_to_cip4: pd.DataFrame,\n",
    "    enrollment: pd.DataFrame\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate diagnostic report showing coverage gaps:\n",
    "    (i) ACS FOD codes not in crosswalk\n",
    "    (ii) CIP4 codes with enrollment but no FOD mapping\n",
    "    (iii) Top unmapped ACS FODs by weighted person-count\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(\"# DIAGNOSTIC REPORT: COVERAGE ANALYSIS\")\n",
    "    print(\"#\"*70)\n",
    "    \n",
    "    # (i) ACS FOD codes not in crosswalk\n",
    "    acs_fods = set(acs['DEGFIELDD'].dropna().unique())\n",
    "    crosswalk_fods = set(fod_to_cip4['FOD'].unique())\n",
    "    missing_fods = acs_fods - crosswalk_fods\n",
    "    \n",
    "    print(f\"\\n(i) ACS FOD codes NOT in crosswalk mapping:\")\n",
    "    print(f\"    Total: {len(missing_fods)} FOD codes\")\n",
    "    if len(missing_fods) > 0:\n",
    "        print(f\"    FODs: {sorted(list(missing_fods))[:20]}\")\n",
    "        # How many ACS observations do these represent?\n",
    "        missing_fod_count = acs[acs['DEGFIELDD'].isin(missing_fods)]['PERWT'].sum()\n",
    "        total_count = acs['PERWT'].sum()\n",
    "        print(f\"    Represents {missing_fod_count:,.0f} / {total_count:,.0f} ACS observations ({missing_fod_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    # (ii) CIP codes with enrollment but no FOD mapping\n",
    "    enrollment_cips = set(enrollment['CIP4'].unique())\n",
    "    crosswalk_cips = set(fod_to_cip4['CIP4'].unique())\n",
    "    unmapped_cips = enrollment_cips - crosswalk_cips\n",
    "    \n",
    "    print(f\"\\n(ii) CIP4 codes with enrollment but NOT mapped from any FOD:\")\n",
    "    print(f\"     Total: {len(unmapped_cips)} CIP4 codes\")\n",
    "    if len(unmapped_cips) > 0:\n",
    "        # Get enrollment counts for these\n",
    "        unmapped_enroll = enrollment[enrollment['CIP4'].isin(unmapped_cips)]\n",
    "        unmapped_2019 = unmapped_enroll[unmapped_enroll['year'] == 2019]['enrollment'].sum()\n",
    "        total_2019 = enrollment[enrollment['year'] == 2019]['enrollment'].sum()\n",
    "        print(f\"     CIP4s: {sorted(list(unmapped_cips))[:30]}\")\n",
    "        print(f\"     2019 enrollment: {unmapped_2019:,.0f} / {total_2019:,.0f} ({unmapped_2019/total_2019*100:.1f}%)\")\n",
    "        print(f\"\\n     Top 10 unmapped CIP4s by 2019 enrollment:\")\n",
    "        top_unmapped = unmapped_enroll[unmapped_enroll['year'] == 2019].nlargest(10, 'enrollment')[['CIP4', 'CIP4_title', 'enrollment']]\n",
    "        for _, row in top_unmapped.iterrows():\n",
    "            print(f\"       CIP4 {row['CIP4']} ({row['CIP4_title']}): {row['enrollment']:,.0f} students\")\n",
    "    \n",
    "    # (iii) NEW: Top unmapped ACS FODs by weighted person-count\n",
    "    print(f\"\\n(iii) Top 20 unmapped ACS FOD codes by weighted person-count:\")\n",
    "    if len(missing_fods) > 0:\n",
    "        unmapped_acs = acs[acs['DEGFIELDD'].isin(missing_fods)]\n",
    "        top_unmapped_fods = unmapped_acs.groupby('DEGFIELDD')['PERWT'].sum().sort_values(ascending=False).head(20)\n",
    "        print(f\"\\n     {'FOD':<8} {'Weighted Count':>15} {'% of Total':>10}\")\n",
    "        print(f\"     {'-'*8} {'-'*15} {'-'*10}\")\n",
    "        for fod, count in top_unmapped_fods.items():\n",
    "            pct = count / total_count * 100\n",
    "            print(f\"     {int(fod):<8} {count:>15,.0f} {pct:>9.2f}%\")\n",
    "    else:\n",
    "        print(\"     All ACS FODs are mapped!\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(\"# AI EXPOSURE AND ENROLLMENT ANALYSIS - 4-DIGIT CIP\")\n",
    "    print(\"#\"*70 + \"\\n\")\n",
    "    \n",
    "    # FILE PATHS\n",
    "    FELTEN_PATH = '/Users/jeffreyohl/Dropbox/CollegeMajorData/FeltenEtAl/2023_Language Modeling AIOE and AIIE.xlsx'\n",
    "    CROSSWALK_PATH = '/Users/jeffreyohl/Dropbox/CollegeMajorData/Crosswalks/crosswalk_handout.xlsx'\n",
    "    ACS_PATH = '/Users/jeffreyohl/Dropbox/CollegeMajorData/IPUMS/usa_00008.csv'\n",
    "    ENROLLMENT_PATH_2025 = '/Users/jeffreyohl/Dropbox/CollegeMajorData/National Student Clearinghouse Data/CTEESpring2025-DataAppendix.xlsx'\n",
    "    ENROLLMENT_PATH_2024 = '/Users/jeffreyohl/Dropbox/CollegeMajorData/National Student Clearinghouse Data/CTEESpring2024-Appendix.xlsx'\n",
    "    OUTPUT_DIR = '/Users/jeffreyohl/Dropbox/CollegeMajorData/output'\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load Felten data\n",
    "        felten = load_felten_data(FELTEN_PATH)\n",
    "        \n",
    "        # Step 2: Load FOD to 4-digit CIP crosswalk (with manual mappings)\n",
    "        fod_to_cip4 = load_fod_cip4_crosswalk(CROSSWALK_PATH, manual_mappings=MANUAL_MAPPINGS)\n",
    "        \n",
    "        # Step 3: Load and combine enrollment data (2019-2025) - MOVED UP!\n",
    "        enrollment = load_and_combine_enrollment_data(ENROLLMENT_PATH_2024, ENROLLMENT_PATH_2025)\n",
    "        \n",
    "        # Step 4: Add empirical enrollment weights to crosswalk\n",
    "        fod_to_cip4_weighted = add_empirical_weights_to_crosswalk(\n",
    "            fod_to_cip4, enrollment, base_year=2019\n",
    "        )\n",
    "        \n",
    "        # Step 5-6: Process ACS and calculate exposure scores\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PROCESSING ACS DATA\")\n",
    "        print(\"=\"*70)\n",
    "        acs = load_and_filter_acs(ACS_PATH)\n",
    "        acs_with_exposure = process_acs_with_exposure(acs, felten, fod_to_cip4_weighted)\n",
    "        cip_exposure = calculate_cip4_exposure(acs_with_exposure)\n",
    "        \n",
    "        # Save exposure scores\n",
    "        cip_exposure.to_csv(f'{OUTPUT_DIR}/cip4_ai_exposure_scores.csv', index=False)\n",
    "        print(f\"\\n✓ Saved exposure scores to {OUTPUT_DIR}/cip4_ai_exposure_scores.csv\")\n",
    "        \n",
    "        # Step 7: Merge enrollment with exposure\n",
    "        df_final = merge_enrollment_exposure(enrollment, cip_exposure)\n",
    "        \n",
    "        # Save final dataset\n",
    "        df_final.to_csv(f'{OUTPUT_DIR}/enrollment_with_ai_exposure_4digit.csv', index=False)\n",
    "        print(f\"\\n✓ Saved final dataset to {OUTPUT_DIR}/enrollment_with_ai_exposure_4digit.csv\")\n",
    "        \n",
    "        # Step 8: Create visualizations\n",
    "        create_descriptive_plots(df_final, f'{OUTPUT_DIR}/enrollment_trends_4digit.png')\n",
    "        \n",
    "        # Step 8B: Create tercile deep-dive plots\n",
    "        create_tercile_deepdive_plots(df_final, f'{OUTPUT_DIR}/enrollment_tercile_deepdive.png')\n",
    "        \n",
    "        # Step 9: Diagnostic reporting - what's missing?\n",
    "        generate_diagnostic_report(acs, fod_to_cip4_weighted, enrollment)\n",
    "        \n",
    "        print(\"\\n\" + \"#\"*70)\n",
    "        print(\"# DATA PREPARATION COMPLETE (4-DIGIT CIP)\")\n",
    "        print(\"#\"*70)\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"1. Review cip4_ai_exposure_scores.csv to validate exposure scores\")\n",
    "        print(\"2. Check enrollment_with_ai_exposure_4digit.csv for data quality\")\n",
    "        print(\"3. Review diagnostic report and add more manual mappings if needed\")\n",
    "        print(\"4. Run econometric_analysis.py for DiD and event study\")\n",
    "        print(\"\\n4-digit CIP analysis provides:\")\n",
    "        print(\"  - Computer Science (1107) vs Information Systems (1104)\")\n",
    "        print(\"  - Business Administration (5202) vs Finance (5208) vs Accounting (5203)\")  \n",
    "        print(\"  - More granular treatment effects and heterogeneity analysis\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ Error: File not found - {e}\")\n",
    "        print(\"\\nPlease check that all data files exist at the specified paths:\")\n",
    "        print(f\"  - Felten: {FELTEN_PATH}\")\n",
    "        print(f\"  - Crosswalk: {CROSSWALK_PATH}\")\n",
    "        print(f\"  - ACS: {ACS_PATH}\")\n",
    "        print(f\"  - Enrollment 2024: {ENROLLMENT_PATH_2024}\")\n",
    "        print(f\"  - Enrollment 2025: {ENROLLMENT_PATH_2025}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
